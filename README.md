# RNA-Seq: Reference Genome, Differential Expression, and Functional Annotation
This repository is a usable, publicly available differential expression and functional annotation tutorial.
All steps have been provided for the UConn CBC Xanadu cluster here with appropriate headers for the Slurm scheduler that can be modified simply to run.  Commands should never be executed on the submit nodes of any HPC machine.  If working on the Xanadu cluster, you should use sbatch scriptname after modifying the script for each stage.  Basic editing of all scripts can be performed on the server with tools, such as nano, vim, or emacs.  If you are new to Linux, please use <a href="https://bioinformatics.uconn.edu/unix-basics/">this</a> handy guide for the operating system commands.  In this guide, you will be working with common bioinformatic file formats, such as <a href="https://en.wikipedia.org/wiki/FASTA_format">FASTA</a>, <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a>, <a href="https://en.wikipedia.org/wiki/SAM_(file_format)">SAM/BAM</a>, and <a href="https://en.wikipedia.org/wiki/General_feature_format">GFF3/GTF</a>. You can learn even more about each file format <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/file-formats-tutorial/">here</a>. If you do not have a Xanadu account and are an affiliate of UConn/UCHC, please apply for one <a href="https://bioinformatics.uconn.edu/contact-us/">here</a>.
	
<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
<li><a href="#First_Point_Header">1 Overview and programs install</>
<li><a href="#Second_Point_Header">2 Accessing the data using sra-toolkit</a></li>
<li><a href="#Third_Point_Header">3 Quality control using sickle</a></li>
<li><a href="#Fourth_Point_Header">4 Aligning reads to a genome using hisat2</a></li>
<li><a href="#Fifth_Point_Header">5 Generating total read counts from alignment using htseq-count</a></li>
<li><a href="#Sixth_Point_Header">6 Pairwise differential expression with counts in R with DESeq2</a></li>
<li><a href="#EnTAP">7 EnTAP: Functional Annotation for Genomes</a></li>
 <li><a href="#Integration">8 Integrating the DE Results with the Annotation Results</a></li>
<li><a href="#Citation">Citations</a></li>
</ul>
</div>

<h2 id="First_Point_Header">Overview and programs install</h2>
In this tutorial we will be analyzing large yellow croaker (Larimichthys crocea) liver samples from the NCBI BioProject (https://www.ncbi.nlm.nih.gov/bioproject/280841) 
Experimental Design: 

Liver mRNA profiles large yellow croaker (Larimichthys crocea) species are sampled during various conditions namely, control group (LB2A), thermal stress group (LC2A), cold stress group (LA2A) and 21-day fasting group (LF1A) were generated by RNA-seq, using Illumina HiSeq 2000. 

We will use the control group (LB2A) and the thermal stress group (LC2A),

If performing headers 1-6 on a personal computer, continue onward. 

If performing headers 1-6 on the Xanadu cluster, it is important that after connecting via SSH the directory is set to

<pre style="color: silver; background: black;">cd /home/CAM/$your.user.name</pre> 

before proceeding. Your home directory contains 10TB of storage and will not pollute the capacities of other users on the cluster. 

The workflow may be cloned into the appropriate directory using the terminal command:
<pre style="color: silver; background: black;">$git clone https://github.com/wolf-adam-eily/refseq_diffexp_funct_annotation_uconn.git
$cd refseq_diffexp_funct_annotation_uconn
$ls  </pre>

If performing headers 1-6 on a local computer, it is recommended the command (in the cloned folder): 
<pre style="color: silver; background: black;">sh -e programs_installation
sh -e r_3.4.3_install
sudo Rscript r_packages_install </pre> 
is run to install <i><b>all</b></i> of the needed software for headers 1-6. If apt-get is not installed on your system, please install that first.
<h2 id="Second_Point_Header">Accessing the data using sra-toolkit </h2>

We will be downloading our data from the sequence-read-archives (SRA), a comprehensive collection of sequenced genetic data submitted to the NCBI by experimenters. The beauty of the SRA is the ease with which genetic data becomes accessible to any scientist with an internet connection, available for download in a variety of formats. Each run in the SRA has a unique identifier. The run may be downloaded using a module of software called the "sratoolkit" and its unique identifier. There are a variety of commands in the sratoolkit, which I invite you to investigate for yourself at https://www.ncbi.nlm.nih.gov/books/NBK158900/.

The data may be accessed at the following web page: 
https://www.ncbi.nlm.nih.gov/bioproject/28084<br>
LB2A : SRR1964642, SRR1964643<br>
LC2A : SRR1964644, SRR1964645<br>

and downloaded with:

<b>xanadu</b><br>
Unlike a local terminal, commands in Xanadu must be concatenated into a single script with the required arguments for the Slurm scheduler. The script is subsequently submitted to the Slurm scheduler using the sbatch command. To initialize a script in Unix we use the nano command, which will produce the following window:
<pre style="color: silver; background: black;">
nano fastq_dumps.sh
 GNU nano 2.3.1            File: fastq_dumps.sh                                








				[ New File ] 
^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos
^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text^T To Spell</pre>

We now write our script with the appropriate <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/xanadu/#Xanadu_6">Slurm arguments</a> followed by our commands:

<pre style="color: silver; background: black;">  GNU nano 2.3.1          File: fastq_dumps.sh                  Modified  
#!/bin/bash
#SBATCH --job-name=fastq_dumps.sh
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o fastq_dumps_%j.out
#SBATCH -e fastq_dumps_%j.err
module load sratoolkit
fastq-dump SRR1964642
mv SRR1964642.fastq LB2A_SRR1964642.fastq
fastq-dump SRR1964643
mv SRR1964643.fastq LB2A_SRR1964643.fastq
				
				[ New File ] 
^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos
^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text ^T To Spell</pre>

We now press CTRL+X which will ask us if we wish to save, simply type "y" to confirm that we do want to save. Next we will be prompted with the file name, we simply hit enter here to save our file (or, if you like, you may change the file name). Now that we have our script, we may run it with the command:

<pre style="color: silver; background: black;">sbatch fastq_dumps.sh</pre>

It is advised that you familiarize yourself with the arguments for the Slurm scheduler. While it may seem as though running your commands locally will be more efficient due to the hassle of not initializing and writing scripts, do not fall for that trap! The capacity of the Slurm scheduler far exceeds the quickness of entering the commands locally. While the rest of this tutorial will not include the process of initializing and writing the Slurm arguments in a script in its coding, know that the Xanadu scripts in the cloned directory <i>do</i> contain the Slurm arguments. However, before running any cloned Xanadu script, you must "nano" and enter your appropriate email address!

<b>local</b>
<pre style="color: silver; background: black;">fastq-dump SRR1964642
fastq-dump SRR1964643</pre>

Unless authorized, you cannot add any packages or software to Xanadu. However, you can see all of the software pre-loaded with the following command:
<pre style="color: silver; background: black;">module avail</pre>

Because of this, it is important to manually load modules to be used in the Xanadu bash. For those on a local computer, the "programs_installation" file installs the software globally and in the executable path, removing any need for loading the module in the terminal.

Now we must repeat the fastq-dump command for SRR1964644 and SRR1964645 samples, or alternatively run either of the following commands in the cloned directory (it is important to use the "nano" command to enter your appropriate email address before running this code on Xanadu): 

<pre style="color: silver; background: black;">sbatch fastq_dump_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e fastqdump_and_trim_local</pre>

The first command will simply download the four fastq files to /home/CAM/your_user_name/refseq_diffexp_funct_annotation_uconn. If proceeding through this headers 1-6 on a personal computer or laptop without access to Xanadu, run the second command. This command will combine the fastq-dump with the next step, quality control: downloading a fastq file, trimming that file, and then removing the untrimmed file. This is recommended if disk space is an issue (the four files combined consume about 75GB of disk space).
Once download is completed, the files were renamed according to the samples for easy identification using the "mv" command. If the first command was run, you should see the following files in your folder: 
<pre style="color: silver; background: black;">|-- LB2A_SRR1964642.fastq
|-- LB2A_SRR1964643.fastq
|-- LC2A_SRR1964644.fastq
|-- LC2A_SRR1964645.fastq</pre>

Let's have a look at the content of one of the fastq-files:

<pre style="color: silver; background: black;">head -n 12 LB2A_SRR1964642.fastq
@SRR1964642.1 FCC355RACXX:2:1101:1476:2162 length=90
CAACATCTCAGTAGAAGGCGGCGCCTTCACCTTCGACGTGGGGAATCGCTTCAACCTCACGGGGGCTTTCCTCTACACGTCCTGTCCGGA
+SRR1964642.1 FCC355RACXX:2:1101:1476:2162 length=90
?@@D?DDBFHHFFGIFBBAFG:DGHDFHGHIIIIC=D<:?BBCCCCCBB@BBCCCB?CCBB<@BCCCAACCCCC>>@?@88?BCACCBB>
@SRR1964642.2 FCC355RACXX:2:1101:1641:2127 length=90
NGCCTGTAAAATCAAGGCATCCCCTCTCTTCATGCACCTCCTGAAATAAAAGGGCCTGAATAATGTCGTACAGAAGACTGCGGCACAGAC
+SRR1964642.2 FCC355RACXX:2:1101:1641:2127 length=90
#1=DDFFFHHHHGJJJJJIIIJIJGIIJJJIJIJJGIJIJJJJIJJJJJJIJJJIJJJJJJJGIIHIGGHHHHHFFFFFDEDBDBDDDDD
@SRR1964642.3 FCC355RACXX:2:1101:1505:2188 length=90
GGACAACGCCTGGACTCTGGTTGGTATTGTCTCCTGGGGAAGCAGCCGTTGCTCCACCTCCACTCCTGGTGTCTATGCCCGTGTCACCGA
+SRR1964642.3 FCC355RACXX:2:1101:1505:2188 length=90CCCFFFFFHHFFHJJJIIIJHHJJHHJJIJIIIJEHJIJDIJJIIJJIGIIIIJGHHHHFFFFFEEEEECDDDDEDEDDDDDDDADDDDD</pre>

We see that for our first three runs we have information about the sampled read including its length followed by the nucleotide read and then a "+" sign. The "+" sign marks the beginning of the corresponding scores for each nucleotide read for the nucleotide sequence preceding the "+" sign. 

<h2 id="Third_Point_Header">Quality control using sickle</h2>

Sickle performs quality control on illumina paired-end and single-end short read data using a sliding window. As the window slides along the fastq file, the average score of all the reads contained in the window is calculated. Should the average window score fall beneath a set threshold, <a href="https://github.com/najoshi/sickle/blob/master/README.md">sickle</a> determines the reads responsible and removes them from the run.

The following command can be applied to each of the four read fastq files:

<b>xanadu (contained within the Slurm script, do not run this alone in the terminal!)</b>
<pre style="color: silver; background: black;">module load sickle
sickle se -f LB2A_SRR1964642.fastq -t sanger -o trimmed_LB2A_SRR1964642.fastq -q 30 -l 50</pre>

<b>local</b>
<pre style="color: silver; background: black;">sickle se -f LB2A_SRR1964642.fastq -t sanger -o trimmed_LB2A_SRR1964642.fastq -q 30 -l 50</pre></pre>

After this point the tutorial will not specify Xanadu or local in its coding excerpts, but assume that the module has been loaded. However, still use the shell scripts for your setups, as they remain differentiated.

The options we use;
<pre style="color: silver; background: black;">Options: 
se    Single end reads
-f    input file name
-t    scoring platform (sanger, illumina, etc.)
-o    output file name
-q    scan the read with the sliding window, cutting when the average quality per base drops below 30 
-l    Removes any reads shorter than 50</pre>

This must be repeated for all four files. If the previous header was run locally, this step has already been performed. Those on Xanadu can run the following shell script to perform the steps:
<pre style="color: silver; background: black;">sbatch fastq_trimming_xanadu.sh</pre>
 
Following the sickle run, the resulting file structure will look as follows:
<pre style="color: silver; background: black;">
|-- trimmed_LB2A_SRR1964642.fastq
|-- trimmed_LB2A_SRR1964643.fastq
|-- trimmed_LC2A_SRR1964644.fastq
|-- trimmed_LC2A_SRR1964645.fastq</pre>
Examine the .out file generated during the run.  It will provide a summary of the quality control process.
<pre style="color: silver; background: black;">Input Reads: 26424138 Surviving: 21799606 (82.50%) Dropped: 4624532 (17.50%)</pre>

<h2 id="Fourth_Point_Header">Aligning reads to a genome using hisat2</h2>
Building an Index:<br>
<a href="https://ccb.jhu.edu/software/hisat2/manual.shtml">HISAT2</a> is a fast and sensitive aligner for mapping next generation sequencing reads against a reference genome.

In order to map the reads to a reference genome, first we must download the reference genome! Then we must make an index file. We will be downloading the reference genome (https://www.ncbi.nlm.nih.gov/genome/12197) from the ncbi database, using the wget command.
<pre style="color: silver; background: black;">wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/972/845/GCF_000972845.1_L_crocea_1.0/GCF_000972845.1_L_crocea_1.0_genomic.fna.gz
gunzip GCF_000972845.1_L_crocea_1.0_genomic.fna.gz</pre>
If you are feeling prudent, you can install the genomic, transcriptomic, and proteomic fastas (yes, all will be used in this tutorial, it is advised you download them now) with the command:
<pre style="color: silver; background: black;">sh -e genomic_and_protein_downloads</pre>
We will use the hisat2-build option to make a HISAT index file for the genome. It will create a set of files with the suffix .ht2, these files together build the index. What is an index and why is it helpful? Genome indexing is the same as indexing a tome, like an encyclopedia. It is much easier to locate information in the vastness of an encyclopedia when you consult the index, which is ordered in an easily navigatable way with pointers to the location of the information you seek within the encylopedia. Genome indexing is thus the structuring of a genome such that it is ordered in an easily navigatable way with pointers to where we can find whichever gene is being aligned. The genome index along with the trimmed fasta files are all you need to align the reads to the reference genome (the build command is included in the genome_indexing_and_alignment* files, so it is not necessary to run now).
<pre style="color: silver; background: black;">hisat2-build -p 4 GCF_000972845.1_L_crocea_1.0_genomic.fna L_crocea

Usage: hisat2-build [options] <reference_in> <bt2_index_base>
reference_in                comma-separated list of files with ref sequences
hisat2_index_base           write ht2 data to files with this dir/basename

Options:
    -p                      number of threads</pre>

After running the command, the following files will be generated as part of the index.  To refer to the index for  mapping the reads in the next step, you will use the file prefix, which in this case is: L_crocea
<pre style="color: silver; background: black;">|-- GCF_000972845.1_L_crocea_1.0_genomic.fna
|-- hisat2_index.sh
|-- L_crocea.1.ht2
|-- L_crocea.2.ht2
|-- L_crocea.3.ht2
|-- L_crocea.4.ht2
|-- L_crocea.5.ht2
|-- L_crocea.6.ht2
|-- L_crocea.7.ht2
|-- L_crocea.8.ht2</pre>

Aligning the reads using HISAT2:<br>
Once we have created the index, the next step is to align the reads with HISAT2 using the index we created. The program will give the output in SAM format. We will not delve into the intricacies of the SAM format here, but it is recommended to peruse https://en.wikipedia.org/wiki/SAM_(file_format) again to garner a greater understanding. We align our reads with the following code:
<pre style="color: silver; background: black;">hisat2 -p 4 --dta -x ../index/L_crocea -q ../quality_control/trim_LB2A_SRR1964642.fastq -S trim_LB2A_SRR1964642.sam

Usage: hisat2 [options]* -x <ht2-idx>  [-S <sam>]
-x <ht2-idx>        path to the Index-filename-prefix (minus trailing .X.ht2) 

Options:
-q                  query input files are FASTQ .fq/.fastq (default)
-p                  number threads
--dta               reports alignments tailored for transcript assemblers</pre>

The above must be repeated for all the files. You may run:
<pre style="color: silver; background: black;">sbatch genome_indexing_and_alignment_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e genome_indexing_and_alignment_local</pre>

to process all four files appropriate for your setup.

Once the mapping have been completed, the file structure is as follows:
<pre style="color: silver; background: black;">
|-- mapping.sh
|-- trim_LB2A_SRR1964642.sam
|-- trim_LB2A_SRR1964643.sam
|-- trim_LC2A_SRR1964644.sam
|-- trim_LC2A_SRR1964645.sam</pre>

When HISAT2 completes its run, it will summarize each of it’s alignments, and it is written to the standard error file, which can be found in the same folder once the run is completed.

<pre style="color: silver; background: black;">
21799606 reads; of these:
  21799606 (100.00%) were unpaired; of these:
    1678851 (7.70%) aligned 0 times
    15828295 (72.61%) aligned exactly 1 time
    4292460 (19.69%) aligned >1 times
92.30% overall alignment rate</pre>

Let's have a look at the SAM file:

<pre style="color: silver; background: black;">head trimmed_LB2A_SRR1964642.sam
@HD VN:1.0 SO:unsorted
@SQ SN:NW_017607850.1 LN:6737
@SQ SN:NW_017607851.1 LN:5396
@SQ SN:NW_017607852.1 LN:5050
@SQ SN:NW_017607853.1 LN:5873
@SQ SN:NW_017607854.1 LN:5692
@SQ SN:NW_017607855.1 LN:11509
@SQ SN:NW_017607856.1 LN:12722
@SQ SN:NW_017607857.1 LN:42555
@SQ SN:NW_017607858.1 LN:11917</pre>

After reading up on the SAM file format, you know that the "@" sign means that we are in the headings section, not the alignment section! The sam file is quite large so there is little purpose in scrolling to find the alignments section (the file is also much too large for using the "grep" command to locate the alignment section). Because of the density of the sam file, it is compressed to binary to create a more easily tractable file for manipulation by future programs. We convert the sam file to b<sub>inary</sub>am with the following command:
<pre style="color: silver; background: black;">samtools view -@ 4 -uhS trim_LB2A_SRR1964642.sam | samtools sort -@ 4 - sort_trim_LB2A_SRR1964642

Usage: samtools [command] [options] in.sam

Command:
view     prints all alignments in the specified input alignment file (in SAM, BAM, or CRAM format) to standard output in SAM format 

Options:
-h      Include the header in the output<br.
-S      Indicate the input was in SAM format
-u      Output uncompressed BAM. This option saves time spent on compression/decompression and is thus preferred when the output is piped to another samtools command
-@      Number of processors</pre>

Usage: samtools [command] [-o out.bam]

Command:
sort    Sort alignments by leftmost coordinates

-o      Write the final sorted output to FILE, rather than to standard output.</pre>

All samples may be run by executing the following command:
<pre style="color: silver; background: black;">sbatch sam_to_bam_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e sam_to_bam_local</pre>
appropriate for your set-up.

Once the conversion is done you will have the following files in the directory.
<pre style="color: silver; background: black;">|-- sort_trim_LB2A_SRR1964642.bam
|-- sort_trim_LB2A_SRR1964643.bam
|-- sort_trim_LC2A_SRR1964644.bam
|-- sort_trim_LC2A_SRR1964645.bam</pre>

<h2 id="Fifth_Point_Header">Generating total read counts from alignent using htseq-count</h2>
Now we will be using the <a href="http://htseq.readthedocs.io/en/master/count.htmhtseq-count">htseq-count</a> function of the module htseq to count the reads which have mapped to the genome. The thought behind htseq-count is quite intuitive, enumerating matching alignments into a "counts" file. However, this belies the complexity of alignment counting. htseq-count is used in the following manner:

<pre style="color: silver; background: black;">htseq-count -s no -r pos -t gene -i Dbxref -f bam ../mapping/sort_trim_LB2A_SRR1964642.bam GCF_000972845.1_L_crocea_1.0_genomic.gff > LB2A_SRR1964642.counts
Usage: htseq-count [options] alignment_file gff_file</pre>

This script takes an alignment file in SAM/BAM format and a feature file in
GFF format and calculates for each feature the number of reads mapping to it.
<pre style="color: silver; background: black;">Options:
  -f SAMTYPE, --format=SAMTYPE
                        type of  data, either 'sam' or 'bam'
                        (default: sam)
  -r ORDER, --order=ORDER
                        'pos' or 'name'. Sorting order of
                        (default: name). Paired-end sequencing data must be
                        sorted either by position or by read name, and the
                        sorting order must be specified. Ignored for single-
                        end data.
  -s STRANDED, --stranded=STRANDED
                        whether the data is from a strand-specific assay.
                        Specify 'yes', 'no', or 'reverse' (default: yes).
                        'reverse' means 'yes' with reversed strand
                        interpretation
  -t FEATURETYPE, --type=FEATURETYPE
                        feature type (3rd column in GFF file) to be used, all
                        features of other type are ignored (default, suitable
                        for Ensembl GTF files: exon)
  -i IDATTR, --idattr=IDATTR
                        GFF attribute to be used as feature ID (default,
                        suitable for Ensembl GTF files: gene_id)</pre>

 
The above command should be repeated for all other BAM files as well. You can process all the BAM files locally with the command:
<pre style="color: silver; background: black;">sh -e htseq_count</pre>
or on Xanadu with (do not forget to nano the script to insert your email):
<pre style="color: silver; background: black;">sbatch htseq_count.sh</pre>
Once all the bam files have been counted, we will be having the following files in the directory.<br>
<pre style="color: silver; background: black;">|-- sort_trim_LB2A_SRR1964642.counts
|-- sort_trim_LB2A_SRR1964643.counts
|-- sort_trim_LC2A_SRR1964644.counts
|-- sort_trim_LC2A_SRR1964645.counts</pre>

Let's have a look at the contents of a counts file:
<pre style="color: silver; background: black;">head sort_trim_LB2A_SRR1964642.counts
GeneID:104917625	18
GeneID:104917626	7
GeneID:104917627	0
GeneID:104917628	199
GeneID:104917629	71
GeneID:104917630	23
GeneID:104917631	111
GeneID:104917632	25
GeneID:104917634	276
GeneID:104917635	254</pre>

We see the layout is quite straightforward, with two columns separated by a tab. The first column identifies the gene from the eponymous sample and the second column is the number of mRNA strands from the row's gene found in the sample. This setup is perfect for our next task, identifying differentially expressed genes.



<h2 id="Sixth_Point_Header">Pairwise differential expression with counts in R using DESeq2</h2>
This part of the tutorial <i>must</i> be run locally. To download the appropriate files to your local computer, we will use the secure copy client, "scp". Close your Xanadu connection and run the following code:
<pre style="color: silver; background: black;">exit
logout
Connection to xanadu-submit-ext.cam.uchc.edu closed.
scp your.user.name@xanadu-submit-ext.cam.uchc.edu:/path/to/counts/&#42;.counts /path/to/local/destination</pre>
Voila! Piece of cake.
To identify differentially expressed genes, We will use the DESeq2 package within Bioconductor in R to perform normalization and statistical analysis of differences among our two sample groups. This R-code is executed in RStudio for R version 3.4.3 (if the r_installation file did not properly install R 3.4.3 you may visit https://linode.com/docs/development/r/how-to-install-r-on-ubuntu-and-debian/ to troubleshoot). Note that Bioconductor will not run on any previous version of R in Linux, so it is imperative that you successfully install R 3.4.3). For our differential expression analysis, we will be using three types of graphs to visualize the data: Bland-Altman (MA), heatmap, and PCA plots. Let's review each plot before diving in:

<b>Bland-Altman(MA) Plot</b>
The Bland-Altman plot is a binary analysis, gathering information on the relationships between two-sets of data. Because we have four sets of data, we will be breaking our data into two separate groups (LB2A and LC2A), then generating two superimposed plots comparing each member of the two groups to each member of the other group. Before we go further, what <i>is</i> a Bland-Altman plot? An MA plot is the comparison of the log-differences for two datasets to the log-averages. That is:

<pre {
	font-family: Cambria,Georgia,serif;
	font-size: 13px;
	font-style: normal;
	font-variant: normal;
	font-weight: 400;
	line-height: 18.57px;
}><strong><i>M</i> = log<sub>2</sub>(D<sub>1</sub>/D<sub>2</sub>) = log<sub>2</sub>(D<sub>1</sub>)-log<sub>2</sub>(D<sub>2</sub>)
<i>A</i> = &frac12;log<sub>2</sub>(D<sub>1</sub>D<sub>2</sub>) = &frac12;(log<sub>2</sub>(D<sub>1</sub>)+log<sub>2</sub>(D<sub>2</sub>))</strong></pre>
with D<sub>1</sub> and D<sub>2</sub> being datasets 1 and 2, respectively. We plot M on the vertical axis and A on the horizontal axis. We see here that should two datapoints from datasets 1 and 2 be equal, they will plot at (D<sub>1</sub>==D<sub>2</sub>, 0). Therefore, should our entire plot run along y=0, it is safe to say, that for us, none of our genes were differentially expressed. However, should there be large deviations from y=0, we are provided with insight that we may have differentially expressed genes.

<b>Heatmap</b>
Heatmaps are the most readily readable visualizations for determining differential expression, unless, like me, you are colorblind! Heatmaps are discrete plots. That is, the values along the x and y axis move in integer increments, with no places in-between. Typically, the x-and-y axes contain different types of information about each dataset, and combine to capture the whole image of the dataset. For us, we may find it useful to make one axis our groups (our four datasets) and the other axis the genes sampled from those datasets. Now each cell in our grid will correspond to the expression of a gene from a sample. We then assign a color-palette to each possible range of gene expression. For instance, an expression between 300-500 may be blue while an expression of 501-1000 may be violet. After coloring all of our cells responsibly, we now have simple, discrete snapshots of each datasets expression distribution. Should we notice that two groups have very similar looking snapshots, we may cluster those groups safely with the assumption that they do not share differentially expressed genes, and therefore belong to the same space. 

<b>Principal Component Analysis(PCA)</b>
Principal-Component-Analysis is far too complex and nuanced to go into detail here. However, you may download an entire tutorial <a href="https://docs.wixstatic.com/ugd/d07f4e_eafc861e1cff411e88b89cf2739e8565.pdf">here</a> for differential expression analysis in R which covers much of what we are about to do, including Principal Component Analysis (pages 15-18, "singular-value-decomposition"). Now! Let's get coding!

<pre style="color: silver; background: black;">library("DESeq2")

&num; Set the working directory
directory <- "~/your_directory_with_htseq_counts"
e_assembly_and_annotation"
setwd(directory)
list.files(directory)

&num; Set the prefix for each output file name
outputPrefix <- "Croaker_DESeq2"

sampleFiles<- c("sort_trim_LB2A_SRR1964642.counts","sort_trim_LB2A_SRR1964643.counts",
                "sort_trim_LC2A_SRR1964644.counts", "sort_trim_LC2A_SRR1964645.counts")

&num; Liver mRNA profiles of 
&num; control group (LB2A), * 
&num; thermal stress group (LC2A), *
sampleNames <- c("LB2A_1","LB2A_2","LC2A_1","LC2A_2")
sampleCondition <- c("control","control","treated","treated")

sampleTable <- data.frame(sampleName = sampleNames,
                          fileName = sampleFiles,
                          condition = sampleCondition)

ddsHTSeq <- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable,
                                       directory = directory,
                                       design = ~ condition)

&num;By default, R will choose a reference level for factors based on alphabetical order. 
&num; To chose the reference we can use: factor()
treatments <- c("control","treated")
ddsHTSeq$condition
&num;Setting the factor levels
colData(ddsHTSeq)$condition <- factor(colData(ddsHTSeq)$condition,
                                      levels = treatments)
ddsHTSeq$condition

&num; Differential expression analysis
&num;differential expression analysis steps are wrapped into a single function, DESeq()
dds <- DESeq(ddsHTSeq)

 
 
&num; results table will be generated using results() which will include:
&num;  log2 fold changes, p values and adjusted p values
res <- results(dds)
res
summary(res)
&num; filter results by p value
res= subset(res, padj<0.05)

&num; order results by padj value (most significant to least)
res <- res[order(res$padj),]
&num; should see DataFrame of baseMean, log2Foldchange, stat, pval, padj

&num; save data results and normalized reads to csv
resdata <- merge(as.data.frame(res), 
                 as.data.frame(counts(dds,normalized =TRUE)), 
                 by = 'row.names', sort = FALSE)
names(resdata)[1] <- 'gene'

write.csv(resdata, file = paste0(outputPrefix, "-results-with-normalized.csv"))</pre>
Let's have a look at our output in the terminal:
Our -results-with-normalized file contains all of the information of our -replaceoutliers-results and additionally the normalized counts of each sample.

<pre style="color: silver; background: black;">head Croaker_DESeq2-results-with-normalized.csv
"","gene","baseMean","log2FoldChange","lfcSE","stat","pvalue","padj","LB2A_1","LB2A_2","LC2A_1","LC2A_2"
"1","GeneID:104917796",4769.95035549771,3.26790212928836,0.0653853095785126,49.9791489916304,0,0,901.106965996412,893.344158166973,8574.41533538478,8710.93496244265
"2","GeneID:104918527",15829.3714065108,1.9381028852635,0.0442492786239224,43.7996492945244,0,0,6560.70781492939,6543.00968591525,25098.6197234442,25115.1484017544
"3","GeneID:104920815",10289.5933862363,1.9899626227926,0.0490973726004825,40.5309391805019,0,0,4107.48228250271,4170.24833394868,16337.3327233147,16543.310205179
"4","GeneID:104921101",3409.31956503535,-4.91775690540935,0.0946450825092183,-51.9599832873553,0,0,6572.16256449714,6628.41731422352,215.779988572597,220.918392848141
"5","GeneID:104921941",4734.72693038149,2.76662808733791,0.0624881461008418,44.27444659461,0,0,1221.83995389344,1204.54206820975,8288.42941751584,8224.09628190694
"6","GeneID:104924020",12853.7020528709,-1.88815134014643,0.047291546513469,-39.9257685431935,0,0,20030.5387441406,20448.7459501297,5466.7705238847,5468.75299332874
"7","GeneID:104924701",2370.03445131097,-3.76035093454651,0.0920739957113332,-40.8405316343152,0,0,4482.62533084656,4345.97207495076,331.413283884228,320.127115562352
"8","GeneID:104925605",11058.8260524593,5.56728188277741,0.0676448603229138,82.3016243392489,0,0,463.917357493916,449.617169714806,21652.3345471029,21669.4351355257
"9","GeneID:104927028",2535.71264215129,-3.30668469336064,0.0834467995056178,-39.6262614378401,0,0,4570.44507753265,4641.46283495983,460.468300973102,470.474355139559</pre>
R
<pre style="color: silver; background: black;">&num; send normalized counts to tab delimited file for GSEA, etc.
write.table(as.data.frame(counts(dds),normalized=T), 
            file = paste0(outputPrefix, "_normalized_counts.txt"), sep = '\t')

&num; produce DataFrame of results of statistical tests
mcols(res, use.names = T)
write.csv(as.data.frame(mcols(res, use.name = T)),
          file = paste0(outputPrefix, "-test-conditions.csv"))</pre>
Let's have a look at our output in the terminal:
<pre style="color: silver; background: black;">head Croaker_DESeq2-test-conditions.csv
"","type","description"
"baseMean","intermediate","mean of normalized counts for all samples"
"log2FoldChange","results","log2 fold change (MLE): condition treated vs control"
"lfcSE","results","standard error: condition treated vs control"
"stat","results","Wald statistic: condition treated vs control"
"pvalue","results","Wald test p-value: condition treated vs control"
"padj","results","BH adjusted p-values</pre>
R
<pre style="color: silver; background: black;">&num; replacing outlier value with estimated value as predicted by distrubution using
&num; "trimmed mean" approach. recommended if you have several replicates per treatment
&num; DESeq2 will automatically do this if you have 7 or more replicates

ddsClean <- replaceOutliersWithTrimmedMean(dds)
ddsClean <- DESeq(ddsClean)
temp_ddsClean <- ddsClean
tab <- table(initial = results(dds)$padj < 0.05,
             cleaned = results(ddsClean)$padj < 0.05)
addmargins(tab)
write.csv(as.data.frame(tab),file = paste0(outputPrefix, "-replaceoutliers.csv"))
</pre>
Let's have a look at our output in the terminal:
<pre style="color: silver; background: black;">less Croaker_DESeq2-replaceoutliers.csv
"","initial","cleaned","Freq"
"1","FALSE","FALSE",12049
"2","TRUE","FALSE",0
"3","FALSE","TRUE",0
"4","TRUE","TRUE",4710</pre>
For this file, we have replaced all outliers with the trimmed-mean of that gene's counts from the other samples in within the same grouping (LB2A, LC2A). Now, we have only four samples, split into our two groups. Therefore, we will be replacing our outliers with the average of the gene count of the group. This approach is best used with a large number of samples belonging to each group, such as eight or above. By changing a value, we are also changing the distribution of data, and thus, the p-values. Our file has three columns, "initial", "cleaned", and "Frequency". The initial column represents if there are any adjusted p-values beneath 0.05 before replacing outliers, the cleaned column representing the same but for after replacing outliers, and the frequency is the amount of outliers in the sample changed. The rows represent our two LB2A samples followed by our two LC2A samples. You can see that changing the outliers in one sample changes the adjusted p-values in <i>both</i> samples. It cannot be stressed enough that this best works with large pools of samples, which we do not have!<br>
R
<pre style="color: silver; background: black;">resClean <- results(ddsClean)
resClean = subset(res, padj<0.05)
resClean <- resClean[order(resClean$padj),]
write.csv(as.data.frame(resClean),file = paste0(outputPrefix, "-replaceoutliers-results.csv"))</pre>
Let's have a look at our output in the terminal:
<pre style="color: silver; background: black;"> head Croaker_DESeq2-replaceoutliers-results.csv
"","baseMean","log2FoldChange","lfcSE","stat","pvalue","padj"
"GeneID:104917796",4769.95035549771,3.26790212928836,0.0653853095785126,49.9791489916304,0,0
"GeneID:104918527",15829.3714065108,1.9381028852635,0.0442492786239224,43.7996492945244,0,0
"GeneID:104920815",10289.5933862363,1.9899626227926,0.0490973726004825,40.5309391805019,0,0
"GeneID:104921101",3409.31956503535,-4.91775690540935,0.0946450825092183,-51.9599832873553,0,0
"GeneID:104921941",4734.72693038149,2.76662808733791,0.0624881461008418,44.27444659461,0,0
"GeneID:104924020",12853.7020528709,-1.88815134014643,0.047291546513469,-39.9257685431935,0,0
"GeneID:104924701",2370.03445131097,-3.76035093454651,0.0920739957113332,-40.8405316343152,0,0
"GeneID:104925605",11058.8260524593,5.56728188277741,0.0676448603229138,82.3016243392489,0,0
"GeneID:104927028",2535.71264215129,-3.30668469336064,0.0834467995056178,-39.6262614378401,0,0</pre>

For this file, there are no separate tabs for the different samples, but only for the aggregrate gene behavior of all samples. Our first column is of course the genes. The second column represents each gene's baseMean. The baseMean is the average of the normalized counts. In normalization, the length of each read does not contribute, only the raw number of total counts. The "log2FoldChange" is simply the scale by which the log2 mean changes after normalization. A "log2FoldChange" value of 2 means that after normalization, the log2 mean value has doubled. A "log2FoldChange" value of -2 means that after normalization, the log2 mean value has halved. Our next tab is the "lfcSE", or "log-fold-change-standard-error". This is simply the <a href="https://en.wikipedia.org/wiki/Standard_error">standard error</a> of the log2 mean. The "stat" tab is the <a href="https://en.wikipedia.org/wiki/Wald_test#Mathematical_details">Wald-test</a> statistic of the log 2 mean. You may think of this statistic as an estimation of the significance of the variable, with smaller values representing greater significance. Lastly, we have our <a href="https://en.wikipedia.org/wiki/P-value">p-values</a> and <a href="https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/anova/supporting-topics/multiple-comparisons/what-is-the-adjusted-p-value/">adjusted-p-values</a>, which are additional measures of significance. We can tell that our Wald-stat, p-values, and adjusted-p-values are quite high, reflecting how the small sample size hurts our assumptions.
R
<pre style="color: silver; background: black;">&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;
&num; Exploratory data analysis of RNAseq data with DESeq2
&num;
&num; these next R scripts are for a variety of visualization, QC and other plots to
&num; get a sense of what the RNAseq data looks like based on DESEq2 analysis
&num;
&num; 1) MA plot
&num; 2) rlog stabilization and variance stabiliazation
&num; 3) PCA plot
&num; 4) heatmap of clustering analysis
&num;
&num;
&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;

&num; MA plot of RNAseq data for entire dataset
&num; http://en.wikipedia.org/wiki/MA_plot
&num; genes with padj < 0.1 are colored Red
plotMA(dds, ylim=c(-8,8),main = "RNAseq experiment")
<img src= "Croaker_DESeq2-MAplot_initial_analysis.png" alt="MA Plot">
dev.copy(png, paste0(outputPrefix, "-MAplot_initial_analysis.png"))
dev.off()

&num; transform raw counts into normalized values
&num; DESeq2 has two options:  1) rlog transformed and 2) variance stabilization
&num; variance stabilization is very good for heatmaps, etc.
rld <- rlogTransformation(dds, blind=T)
vsd <- varianceStabilizingTransformation(dds, blind=T)

&num; save normalized values
write.table(as.data.frame(assay(rld),file = paste0(outputPrefix, "-rlog-transformed-counts.txt"), sep = '\t'))
write.table(as.data.frame(assay(vsd),file = paste0(outputPrefix, "-vst-transformed-counts.txt"), sep = '\t'))


&num; clustering analysis
&num; excerpts from http://dwheelerau.com/2014/02/17/how-to-use-deseq2-to-analyse-rnaseq-data/
library("RColorBrewer")
library("gplots")
sampleDists <- dist(t(assay(rld)))
suppressMessages(library("RColorBrewer"))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(colnames(rld), rld$type, sep="")
colnames(sampleDistMatrix) <- paste(colnames(rld), rld$type, sep="")
colors <- colorRampPalette( rev(brewer.pal(8, "Blues")) )(255)
heatmap(sampleDistMatrix,col=colors,margin = c(8,8))
<img src= "Croaker_DESeq2-clustering.png" alt = "Cluster Heatmap">
dev.copy(png,paste0(outputPrefix, "-clustering.png"))
dev.off()

&num;Principal components plot shows additional but rough clustering of samples
library("genefilter")
library("ggplot2")
library("grDevices")

rv <- rowVars(assay(rld))
select <- order(rv, decreasing=T)[seq_len(min(500,length(rv)))]
pc <- prcomp(t(assay(vsd)[select,]))

&num; set condition
condition <- treatments
scores <- data.frame(pc$x, condition)

(pcaplot <- ggplot(scores, aes(x = PC1, y = PC2, col = (factor(condition))))
  + geom_point(size = 5)
  + ggtitle("Principal Components")
  + scale_colour_brewer(name = " ", palette = "Set1")
  + theme(
    plot.title = element_text(face = 'bold'),
    legend.position = c(.9,.2),
    legend.key = element_rect(fill = 'NA'),
    legend.text = element_text(size = 10, face = "bold"),
    axis.text.y = element_text(colour = "Black"),
    axis.text.x = element_text(colour = "Black"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = 'bold'),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.background = element_rect(color = 'black',fill = NA)
  ))
<img src="Croaker_DESeq2-ggplot2.png" alt= "PCA Plot">
&num;dev.copy(png,paste0(outputPrefix, "-PCA.png"))
ggsave(pcaplot,file=paste0(outputPrefix, "-ggplot2.png"))



&num; heatmap of data
library("RColorBrewer")
library("gplots")
par(mar=c(7,4,4,2)+0.1) 
&num; 1000 top expressed genes with heatmap.2
select <- order(rowMeans(counts(ddsClean,normalized=T)),decreasing=T)[1:100]
my_palette <- colorRampPalette(c("blue",'white','red'))(n=100)
heatmap.2(assay(vsd)[select,], col=my_palette,
          scale="row", key=T, keysize=1, symkey=T,
          density.info="none", trace="none",
          cexCol=0.6, labRow=F,
          main="Heatmap of 100 DE Genes \nin Liver Tissue Comparison")
dev.copy(png, paste0(outputPrefix, "-HEATMAP.png"))
dev.off()</pre>

It is recommended the user study and attempt the code on one's own before moving onward. The resulting files are located in the directory.

<h2 id = "EnTAP">Entap: Functional Annotation for Transcriptomes</h2>

Unfortunately for those proceeding through this tutorial locally, an HPC is required to complete the annotation. We will be using the program EnTAP, which serves as a functional annotation for transcriptomes. The installation for EnTAP has not been included in this tutorial as the software must be installed on the server through which one wishes to run her or his commands. The installation file <i>does</i>, however, contain all of the dependcies, with the exception of the InterProScan and EGGNOG-MAPPER databases due to their sizes.

For robust transcriptomic data, functional annotation is a good avenue through which to begin annotating genomes. The core step of functional annotation is creating alignments to a genomic-data-base, retrieving the function and ontology of the alignments to be used to annotate the genome of interest. To do this, a genomic-data-base must be curated, indexed, followed by a series of filtering (contaminant detection, assembly artifact removal) and our alignment steps. While we will not be covering the ins-and-outs of EnTAP specifically here, you may visit http://entap.readthedocs.io/en/latest/introduction.html for insight on the methodology of the software.

Before running EnTAP we must complete a series of needed local 'warm-up' tasks first, starting with retrieving the protein IDs of our differentially expressed genes. Let's begin this objective by moving our first 9 differentially expressed genes from our Croaker_DESeq2-results-with-normalized.csv file to a temporary file, temp.csv, then use those 9 gene ids to retrieve the 9 corresponding protein ids and AA-sequences from the protein table found at https://www.ncbi.nlm.nih.gov/genome/proteins/12197?genome_assembly_id=229515 (this must be downloaded now). To do this, we will use csvgeneID2fasta.py with the following code:

<pre style="color: silver; background: black;">head -n 10 Croaker_DESeq2-results-with-normalized.csv > temp.csv 
python csvgeneID2fasta.py
</pre>

Which will prompt us with a series of questions we must answer:

<pre style="color: silver; background: black;">Please enter the file destination containing your differentially expressed Gene IDs 
temp.csv
Please enter the file destination containing the appropriate NIH protein table 
ProteinTable12197_229515.txt
Please enter the file destination of your protein fasta 
GCF_000972845.1_L_crocea_1.0_protein.faa
Please enter your desired fasta output destination 
fasta_out.fasta</pre>

After generating our new fasta file, we must now create the databases against which we will be searching for our annotations. We will be using two sources for our databases: the Uniprot-Swissprot fasta, and the <a href="ftp://ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other/">vertebrate_other</a> amino-acd fastas provided through NCBI's RefSeq release. The Uniprot-Swissprot database is already loaded onto the Xanadu server. However, the "vertebrate_other" fastas are not, so we will be retrieving and compiling the RefSeq database ourselves. Looking at the link we see that there are four types of files for each index. Because our "fasta_out" file has protein sequences, we are only interested in the amino acid fastas, the 'faa.gz' files. We may use the '-A' argument of wget (along with other arguments I encourage you to look up) to select only the amino acid fastas. To do this we run:

<pre style="color: silver; background: black;">wget -A faa.gz -m -p -E -k -K -np ftp://ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other/</pre>

Now we must compile all of the fastas into a single fasta:

<pre style="color: silver; background: black;">cd ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other
gunzip &#42;.faa.gz
cat &#42;.faa > vertebrate_other_fasta.txt | tr '\n' ''</pre>

We use the UNIX 'tr' function to remove new lines. The reason for this will become evident with experience, but the RefSeq fastas are compiled in such a way that when concatenating, blank lines are appended between the ends and beginning of each file. These blank lines cause errors when creating our database, so it is necessary to remove them.

Now we must create scannable databases using the software <a href="https://github.com/bbuchfink/diamond">DIAMOND</a>. DIAMOND is renowned for the efficiency and speed with which it not only creates scannable databases, but also its alignment matching against those databases. DIAMOND's efficiency is such that it is runnable on most laptops and personal computers, which is truly quite marvelous considering its resource intensive alternative, BLAST. We initialize a script with our "nano" command and Slurm arguments with the coding portion reading:

<pre style="color: silver; background: black;">module load diamond
wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accesion2taxid/prot.accession2taxid.gz
wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip
diamond makedb --in vertebrate_other.fasta -d vertebrate_other --taxonmap prot.accession2taxid.gz --taxonnodes taxdmp.zip</pre>

makedb is run with the options --taxonmap and --taxonnodes to structure the databases such that should we prefer alignments made to a specific clade, such as chordates, the database contains the appropriate information to do as we ask.

Now we have all of the ingredients to write our EnTAP script. Because <a href="https://bioinformatics.uconn.edu/databases/">Xanadu contains the databases</a>, it is necessary only to secure copy the fasta_out.fasta file to our appropriate Xanadu directory. However, Xanadu does not have specifically the vertebrate_only.dmnd database, but rather the complete_protein.dmnd database. This is no problem, as we will signal to EnTAP we are only interested in chordate alignments:

<pre style="color: silver; background: black;">nano entap.sh


Required Flags:
--runP      with a protein input, frame selection will not be ran and annotation will be executed with protein sequences (blastp)
-i          Path to the transcriptome file (either nucleotide or protein)
-d          Specify up to 5 DIAMOND indexed (.dmnd) databases to run similarity search against

Optional:
-threads    Number of threads
--ontology  0 - EggNOG (default)</pre>

We now submit our EnTAP script to the Slurm scheduler and wait for the process to finish. Once the job is done it will create a folder called “outfiles” which will contain the output of the program:

<pre style="color: silver; background: black;">entap/
|-- outfiles/
|   |-- debug_2018.2.25-2h49m29s.txt
|   |-- entap_out/
|   |-- final_annotated.faa
|   |-- final_annotations_lvl0_contam.tsv
|   |-- final_annotations_lvl0_no_contam.tsv
|   |-- final_annotations_lvl0.tsv
|   |-- final_annotations_lvl3_contam.tsv
|   |-- final_annotations_lvl3_no_contam.tsv
|   |-- final_annotations_lvl3.tsv
|   |-- final_annotations_lvl4_contam.tsv
|   |-- final_annotations_lvl4_no_contam.tsv
|   |-- final_annotations_lvl4.tsv
|   |-- final_unannotated.faa
|   |-- final_unannotated.fnn
|   |-- log_file_2018.2.25-2h49m29s.txt
|   |-- ontology/
|   |-- similarity_search/</pre>

<h2 id = "Integration">Integrating the DE Results with the Annotation Results</h2>

You must copy the following two files from the EnTAP run to your computer using Globus Peronsal Connect: “GeneID_proteinID.txt” and “final_annotations_lvl0_contam.tsv” file from the previous run. The two files may be found in:

<pre style="color: silver; background: black;">
entap/
|-- GeneID_proteinID.txt
|-- outfiles/
|   |-- final_annotations_lvl0_contam.tsv</pre>



Now, lastly, we integrate the annotations with the DE genes using the following R code:

<pre style="color: silver; background: black;">library("readr")
&num;read the csv file with DE genes
csv <- read.csv("Croaker_DESeq2-results-with-normalized.csv")
&num;read the file with geneID to proteinID relationship
gene_protein_list <- read.table("GeneID_proteinID.txt")
names(gene_protein_list) <- c('GeneID','table', 'tableID','protein', 'protienID')
gene_protein_list <- gene_protein_list[,c("GeneID","protienID")]

&num;merging the two dataframes
DEgene_proteinID <- merge(csv, gene_protein_list, by.x="gene", by.y="GeneID")

&num;read_tsv
annotation_file <- read_tsv('final_annotations_lvl0.tsv', col_names = TRUE)
names(annotation_file)[1] <- 'query_seqID'

&num;merging the DEgene_proteinID with annotation_file dataframes
annotated_DEgenes <- merge(DEgene_proteinID, annotation_file, by.x="protienID", by.y="query_seqID")
View(annotated_DEgenes)
write.csv(annotated_DEgenes, file = paste0("annotated_DEgenes_final.csv"))</pre>

Congratulations on completing your differential expression and functional annotation tutorial!

<h2 id="Citation">Citations</h2>

Alex Hart, Jill Wegrzyn http://entap.readthedocs.io/en/latest/index.html

Anders, Simon, Paul Theodor Pyl, and Wolfgang Huber. “HTSeq—a Python Framework to Work with High-Throughput Sequencing Data.” Bioinformatics 31.2 (2015): 166–169. PMC. Web. 8 Mar. 2018.

B. Buchfink, Xie C., D. Huson, "Fast and sensitive protein alignment using DIAMOND", Nature Methods 12, 59-60 (2015).

E. Neuwirth, RColorBrewer https://cran.r-project.org/web/packages/RColorBrewer/index.html

Gentleman R, Carey V, Huber W and Hahne F (2017). genefilter: genefilter: methods for filtering genes from high-throughput experiments. R package version 1.60.0. 

Gregory R. Warnes, Ben Bolker, Lodewijk Bonebakker, Robert Gentleman, Wolfgang Huber Andy Liaw, Thomas Lumley, Martin Maechler, Arni Magnusson, Steffen Moeller, Marc Schwartz, Bill Venables, gplots https://cran.r-project.org/web/packages/gplots/index.html

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009. 

Joshi NA, Fass JN. (2011). Sickle: A sliding-window, adaptive, quality-based trimming tool for FastQ files 
(Version 1.33) [Software].  Available at https://github.com/najoshi/sickle.

Leinonen, Rasko, Hideaki Sugawara, and Martin on behalf of the International Nucleotide Sequence Database Collaboration. “The Sequence Read Archive.” Nucleic Acids Research 39.Database issue (2011): D19–D21. PMC. Web. 8 Mar. 2018.

Li H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R, and 1000 Genome Project Data Processing Subgroup, The Sequence alignment/map (SAM) format and SAMtools, Bioinformatics (2009) 25(16) 2078-9

Love MI, Huber W and Anders S (2014). “Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2.” Genome Biology, 15, pp. 550. doi: 10.1186/s13059-014-0550-8. 
