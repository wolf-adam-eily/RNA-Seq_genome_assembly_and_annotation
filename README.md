# RNA-Seq: Reference Genome, Differential Expression, and Functional Annotation
This repository is a usable, publicly available differential expression and functional annotation tutorial. The objective of this tutorial is beyond familiarizing you with how to use bioinformatics tools, but to also begin thinking like an out-of-the-box bioinformatics problem solver. Because of this, there is considerable mathematical and statistical content in the reading. However, do not be alarmed, as clear explanations for all computational content is promptly provided. By the end of this tutorial you should be well-versed in using bioinformatics tools and taking computational approaches to difficult biological problems.
All steps have been provided for the UConn CBC Xanadu cluster here with appropriate headers for the Slurm scheduler that can be modified simply to run.  Commands should never be executed on the submit nodes of any HPC machine.  If working on the Xanadu cluster, you should use sbatch scriptname after modifying the script for each stage.  Basic editing of all scripts can be performed on the server with tools, such as nano, vim, or emacs.  If you are new to Linux, please use <a href="https://bioinformatics.uconn.edu/unix-basics/">this</a> handy guide for the operating system commands.  In this guide, you will be working with common bioinformatic file formats, such as <a href="https://en.wikipedia.org/wiki/FASTA_format">FASTA</a>, <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a>, <a href="https://en.wikipedia.org/wiki/SAM_(file_format)">SAM/BAM</a>, and <a href="https://en.wikipedia.org/wiki/General_feature_format">GFF3/GTF</a>. You can learn even more about each file format <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/file-formats-tutorial/">here</a>. If you do not have a Xanadu account and are an affiliate of UConn/UCHC, please apply for one <a href="https://bioinformatics.uconn.edu/contact-us/">here</a>.
	
<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
<li><a href="#First_Point_Header">1 Overview and programs install</>
<li><a href="#Second_Point_Header">2 Accessing the data using sra-toolkit</a></li>
<li><a href="#Third_Point_Header">3 Quality control using sickle</a></li>
<li><a href="#Fourth_Point_Header">4 Aligning reads to a genome using hisat2</a></li>
<li><a href="#Fifth_Point_Header">5 Generating total read counts from alignment using htseq-count</a></li>
<li><a href="#Sixth_Point_Header">6 Pairwise differential expression with counts in R with DESeq2</a></li>
	<ol><li><a href="#types_of_plots">1 Common plots for differential expression analysis</a></li>
		<li><a href="#using_deseq2">2 Using DESeq2</a></li></ol>
<li><a href="#EnTAP">7 EnTAP: Functional Annotation for Genomes</a></li>
 <li><a href="#Integration">8 Integrating the DE Results with the Annotation Results</a></li>
<li><a href="#Citation">Citations</a></li>
</ul>
</div>

<h2 id="First_Point_Header">Overview and programs install</h2>
In this tutorial we will be analyzing large yellow croaker (Larimichthys crocea) liver samples from the NCBI BioProject (https://www.ncbi.nlm.nih.gov/bioproject/280841) 
Experimental Design: 

Liver mRNA profiles large yellow croaker (Larimichthys crocea) species are sampled during various conditions namely, control group (LB2A), thermal stress group (LC2A), cold stress group (LA2A) and 21-day fasting group (LF1A) were generated by RNA-seq, using Illumina HiSeq 2000. 

We will use the control group (LB2A) and the thermal stress group (LC2A),

If performing headers 1-6 on a personal computer, continue onward. 

If performing headers 1-6 on the Xanadu cluster, it is important that after connecting via SSH the directory is set to

<pre style="color: silver; background: black;">cd /home/CAM/$USER/</pre> 

before proceeding. Your home directory contains 10TB of storage and will not pollute the capacities of other users on the cluster. 

The workflow may be cloned into the appropriate directory using the terminal command:
<pre style="color: silver; background: black;">-bash-4.2$ git clone https://github.com/wolf-adam-eily/refseq_diffexp_funct_annotation_uconn.git
-bash-4.2$ cd refseq_diffexp_funct_annotation_uconn
-bash-4.2$ ls  </pre>

If performing headers 1-6 on a local computer, it is recommended the command (in the cloned folder): 
<pre style="color: silver; background: black;">sh -e programs_installation
sh -e r_3.4.3_install
sudo Rscript r_packages_install </pre> 
is run to install <i><b>all</b></i> of the needed software for headers 1-6. If apt-get is not installed on your system, please install that first.
<h2 id="Second_Point_Header">Accessing the data using sra-toolkit </h2>

We will be downloading our data from the sequence-read-archives (SRA), a comprehensive collection of sequenced genetic data submitted to the NCBI by experimenters. The beauty of the SRA is the ease with which genetic data becomes accessible to any scientist with an internet connection, available for download in a variety of formats. Each run in the SRA has a unique identifier. The run may be downloaded using a module of software called the "sratoolkit" and its unique identifier. There are a variety of commands in the sratoolkit, which I invite you to investigate for yourself at https://www.ncbi.nlm.nih.gov/books/NBK158900/.

The data may be accessed at the following web page: 
https://www.ncbi.nlm.nih.gov/bioproject/28084<br>
LB2A : SRR1964642, SRR1964643<br>
LC2A : SRR1964644, SRR1964645<br>

and downloaded with:

<b>xanadu</b><br>
Unlike a local terminal, commands in Xanadu must be concatenated into a single script with the required arguments for the Slurm scheduler. The script is subsequently submitted to the Slurm scheduler using the sbatch command. To initialize a script in Unix we use the nano command, which will produce the following window:
<pre style="color: silver; background: black;">
nano fastq_dumps.sh
 GNU nano 2.3.1            File: fastq_dumps.sh                                








				[ New File ] 
^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos
^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text^T To Spell</pre>

We now write our script with the appropriate <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/xanadu/#Xanadu_6">Slurm arguments</a> followed by our commands:

<pre style="color: silver; background: black;">  GNU nano 2.3.1          File: fastq_dumps.sh                  Modified  
#!/bin/bash
#SBATCH --job-name=fastq_dumps.sh
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o fastq_dumps_%j.out
#SBATCH -e fastq_dumps_%j.err
module load sratoolkit
fastq-dump SRR1964642
mv SRR1964642.fastq LB2A_SRR1964642.fastq
fastq-dump SRR1964643
mv SRR1964643.fastq LB2A_SRR1964643.fastq
				
				[ New File ] 
^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos
^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text ^T To Spell</pre>

We now press CTRL+X which will ask us if we wish to save, simply type "y" to confirm that we do want to save. Next we will be prompted with the file name, we simply hit enter here to save our file (or, if you like, you may change the file name). Now that we have our script, we may run it with the command:

<pre style="color: silver; background: black;">-bash-4.2$ sbatch fastq_dumps.sh</pre>

It is advised that you familiarize yourself with the arguments for the Slurm scheduler. While it may seem as though running your commands locally will be more efficient due to the hassle of not initializing and writing scripts, do not fall for that trap! The capacity of the Slurm scheduler far exceeds the quickness of entering the commands locally. While the rest of this tutorial will not include the process of initializing and writing the Slurm arguments in a script in its coding, know that the Xanadu scripts in the cloned directory <i>do</i> contain the Slurm arguments. However, before running any cloned Xanadu script, you must "nano" and enter your appropriate email address!

<b>local</b>
<pre style="color: silver; background: black;">fastq-dump SRR1964642
fastq-dump SRR1964643</pre>

Unless authorized, you cannot add any packages or software to Xanadu. However, you can see all of the software pre-loaded with the following command:
<pre style="color: silver; background: black;">module avail</pre>

Because of this, it is important to manually load modules to be used in the Xanadu bash. For those on a local computer, the "programs_installation" file installs the software globally and in the executable path, removing any need for loading the module in the terminal.

Now we must repeat the fastq-dump command for SRR1964644 and SRR1964645 samples, or alternatively run either of the following commands in the cloned directory (it is important to use the "nano" command to enter your appropriate email address before running this code on Xanadu): 

<pre style="color: silver; background: black;">-bash-4.2$ sbatch fastq_dump_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e fastqdump_and_trim_local</pre>

The first command will simply download the four fastq files to /home/CAM/your_user_name/refseq_diffexp_funct_annotation_uconn. If proceeding through this headers 1-6 on a personal computer or laptop without access to Xanadu, run the second command. This command will combine the fastq-dump with the next step, quality control: downloading a fastq file, trimming that file, and then removing the untrimmed file. This is recommended if disk space is an issue (the four files combined consume about 75GB of disk space).
Once download is completed, the files were renamed according to the samples for easy identification using the "mv" command. If the first command was run, you should see the following files in your folder: 
<pre style="color: silver; background: black;">-bash-4.2$ ls &#42;fastq
<strong>LB2A_SRR1964642.fastq
LB2A_SRR1964643.fastq
LC2A_SRR1964644.fastq
LC2A_SRR1964645.fastq</strong></pre>

Let's have a look at the content of one of the fastq-files:

<pre style="color: silver; background: black;">-bash-4.2$ head -n 12 LB2A_SRR1964642.fastq
@SRR1964642.1 FCC355RACXX:2:1101:1476:2162 length=90
CAACATCTCAGTAGAAGGCGGCGCCTTCACCTTCGACGTGGGGAATCGCTTCAACCTCACGGGGGCTTTCCTCTACACGTCCTGTCCGGA
+SRR1964642.1 FCC355RACXX:2:1101:1476:2162 length=90
?@@D?DDBFHHFFGIFBBAFG:DGHDFHGHIIIIC=D<:?BBCCCCCBB@BBCCCB?CCBB<@BCCCAACCCCC>>@?@88?BCACCBB>
@SRR1964642.2 FCC355RACXX:2:1101:1641:2127 length=90
NGCCTGTAAAATCAAGGCATCCCCTCTCTTCATGCACCTCCTGAAATAAAAGGGCCTGAATAATGTCGTACAGAAGACTGCGGCACAGAC
+SRR1964642.2 FCC355RACXX:2:1101:1641:2127 length=90
#1=DDFFFHHHHGJJJJJIIIJIJGIIJJJIJIJJGIJIJJJJIJJJJJJIJJJIJJJJJJJGIIHIGGHHHHHFFFFFDEDBDBDDDDD
@SRR1964642.3 FCC355RACXX:2:1101:1505:2188 length=90
GGACAACGCCTGGACTCTGGTTGGTATTGTCTCCTGGGGAAGCAGCCGTTGCTCCACCTCCACTCCTGGTGTCTATGCCCGTGTCACCGA
+SRR1964642.3 FCC355RACXX:2:1101:1505:2188 length=90CCCFFFFFHHFFHJJJIIIJHHJJHHJJIJIIIJEHJIJDIJJIIJJIGIIIIJGHHHHFFFFFEEEEECDDDDEDEDDDDDDDADDDDD</pre>

We see that for our first three runs we have information about the sampled read including its length followed by the nucleotide read and then a "+" sign. The "+" sign marks the beginning of the corresponding scores for each nucleotide read for the nucleotide sequence preceding the "+" sign. 

<h2 id="Third_Point_Header">Quality control using sickle</h2>

<pre style="color: silver; background: black;">-bash-4.2$ module load sickle

-bash-4.2$ sickle

<strong>Usage</strong>: sickle <command> [options]

<strong>Command</strong>:
pe	paired-end sequence trimming
se	single-end sequence trimming

--help, display this help and exit
--version, output version  Information and exit</pre>

We have single-end sequences. 

<pre style="color: silver; background: black;">-bash-4.2$ sickle se

<strong>Usage</strong>: sickle se [options] -f <fastq sequence file> -t <quality type> -o <trimmed fastq file>

<strong>Options</strong>:
-f, --fastq-file, Input fastq file (required)
-t, --qual-type, Type of quality values (solexa (CASAVA < 1.3), illumina (CASAVA 1.3 to 1.7), sanger (which is CASAVA >= 1.8)) (required)
-o, --output-file, Output trimmed fastq file (required)
-q, --qual-threshold, Threshold for trimming based on average quality in a window. Default 20.
-l, --length-threshold, Threshold to keep a read based on length after trimming. Default 20.
-x, --no-fiveprime, Don't do five prime trimming.
-n, --trunc-n, Truncate sequences at position of first N.
-g, --gzip-output, Output gzipped files.
--quiet, Don't print out any trimming  Information
--help, display this help and exit
--version, output version  Information and exit</pre>

The quality may be any score from 0 to 40. The default of 20 is much too low for a robust analysis. We want to select only reads with a quality of 35 or better. Additionally, the desired length of each read is 50bp. Again, we see that a default of 20 is much too low for analysis confidence. We want to select only reads whose lengths exceed 45bp. Lastly, we must know the scoring type. While the quality type is not listed on the SRA pages, most SRA reads use the "sanger" quality type. Unless explicitly stated, try running sickle using the sanger qualities. If an error is returned, try illumina. If another error is returned, lastly try solexa.

Let's put all of this together for our sickle script using our downloaded fastq files:

<b>xanadu (contained within the Slurm script, do not run this alone in the terminal!)</b>
<pre style="color: silver; background: black;">module load sickle
sickle se -f LB2A_SRR1964642.fastq -t sanger -o trimmed_LB2A_SRR1964642.fastq -q 30 -l 50</pre>

<b>local</b>
<pre style="color: silver; background: black;">sickle se -f LB2A_SRR1964642.fastq -t sanger -o trimmed_LB2A_SRR1964642.fastq -q 30 -l 50</pre></pre>

After this point the tutorial will not specify Xanadu or local in its coding excerpts, but assume that the module has been loaded. However, still use the shell scripts for your setups, as they remain differentiated.

This must be repeated for all four files. If the previous header was run locally, this step has already been performed. Those on Xanadu can run the following shell script to perform the steps:
<pre style="color: silver; background: black;">-bash-4.2$ sbatch fastq_trimming_xanadu.sh</pre>
 
Following the sickle run, the resulting file structure will look as follows:
<pre style="color: silver; background: black;">-bash-4.2$ ls trimmed&#42;							
<strong>trimmed_LB2A_SRR1964642.fastq
trimmed_LB2A_SRR1964643.fastq
trimmed_LC2A_SRR1964644.fastq
trimmed_LC2A_SRR1964645.fastq</strong></pre>
Examine the .out file generated during the run.  It will provide a summary of the quality control process.
<pre style="color: silver; background: black;">Input Reads: 26424138 Surviving: 21799606 (82.50%) Dropped: 4624532 (17.50%)</pre>

It is helpful to see how the quality of the data has changed after using sickle. To do this, we will be using the commandline versions of <a href="https://www.bio Informatics.babraham.ac.uk/projects/fastqc/INSTALL.txt">fastqc</a> and <a href="http://multiqc. Info/docs/">MultiQC</a>. These two programs simply create reports of the average quality of our trimmed reads, with some graphs. There is no way to view a --help menu for these programs in the command-line. However, their use is quite simple, we simply run "fastqc <trimmed_fastq>" or "multiqc -f -n trimmed trimmed*". Do not worry too much about the options for MultiQC! Let's write our script:

<pre style="color: silver; background: black;">-bash-4.2$ nano quality_control.sh

  GNU nano 2.3.1                                                    File: quality_control.sh                                                                                                                

#!/bin/bash
#SBATCH --job-name=quality_control
#SBATCH --mail-user=
#SBATCH --mail-type=ALL
#SBATCH -n 1
#SBATCH -N 1
#SBATCH -c 16
#SBATCH --mem=120G
#SBATCH -o quality_control_%j.out
#SBATCH -e quality_control_%j.err
#SBATCH --partition=general

export TMPDIR=/home/CAM/$USER/tmp/

module load fastqc
module load MultiQC

fastqc trimmed_LB2A_SRR1964642.fastq
fastqc trimmed_LB2A_SRR1964643.fastq
fastqc trimmed_LC2A_SRR1964644.fastq
fastqc trimmed_LC2A_SRR1964645.fastq
multiqc -f -n trimmed trimmed*



                                                                                             [ Read 26 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell
</pre>
<br>
<pre style="color: silver; background: black;">-bash-4.2$ sbatch quality_control.sh</pre>

fastqc will create the files "trimmed_file_fastqc.html". To have a look at one, we need to move all of our "trimmed_file_fastqc.html" files into a single directory, and then <a href="https://www.techrepublic.com/article/how-to-use-secure-copy-for-file-transfer/">secure copy</a> that folder to our local directory. Then, we may open our files! If that seems like too much work for you, you may open the files directly through this github. Simply click on any "html" file and you may view it in your browser immediately. Because of this, the steps mentioned above will not be placed in this tutorial.

This script will also create a directory "trimmed_data". Let's look inside of that directory:

<pre style="color: silver; background: black;">-bash-4.2$ cd trimmed_data
-bash-4.2$ ls 
<strong>multiqc_fastqc.txt         multiqc.log
multiqc_general_stats.txt  multiqc_sources.txt
</strong></pre>

Let's have a look at the file format from fastqc and multiqc. When loading the fastqc file, you will be greeted with this screen:
<img src="images/fastqc1.png">

There are some basic statistics which are all pretty self-explanatory. Notice that none of our sequences fail the quality report! It would be concerning if we had even one because this report is from our trimmed sequence! The same thinking applies to our sequence length. Should the minimum of the sequence length be below 45, we would know that sickle had not run properly. Let's look at the next index in the file:
<img src="images/fastqc2.png">

This screen is simply a <a href="https://en.wikipedia.org/wiki/Box_plot">box-and-whiskers plot</a> of our quality scores per base pair. Note that there is a large variance and lower mean scores (but still about in our desired range) for base pairs 1-5. These are the primer sequences! I will leave it to you to ponder the behavior of this graph. If you're stumped, you may want to learn how <a href="https://www.illumina.com/techniques/sequencing.html">Illumina sequencing"</a> works.

Our next index is the per sequence quality scores:
<img src="images/fastqc3.png">

This index is simply the total number of base pairs (y-axis) which have a given quality score (x-axis). This plot is discontinuous and discrete, and should you calculate the <a href="https://en.wikipedia.org/wiki/Riemann_sum">Riemann sum</a> the result is the total number of base pairs present across all reads.
	
The last index at which we are going to look is the "Overrepresented Sequences" index:
<img src="images/fastqc4.png">
This is simply a list of sequences which appear disproportionately in our reads file. The reads file actually includes the primer sequences for this exact reason. When fastqc calculates a sequence which appears many times beyond the expected distribution, it may check the primer sequences in the reads file to determine if the sequence is a primer. If the sequence is not a primer, the result will be returned as "No Hit". Sequences which are returned as "No Hit" are most likely highly expressed genes.

We see that our multiqc file has the same indices as our fastqc files, but is simply the mean of all the statistics across our fastqc files:
<img src="images/multiqc.png">

<h2 id="Fourth_Point_Header">Aligning reads to a genome using hisat2</h2>
Building an Index:<br>
<a href="https://ccb.jhu.edu/software/hisat2/manual.shtml">HISAT2</a> is a fast and sensitive aligner for mapping next generation sequencing reads against a reference genome.

In order to map the reads to a reference genome, first we must download the reference genome! Then we must make an index file. We will be downloading the reference genome (https://www.ncbi.nlm.nih.gov/genome/12197) from the ncbi database, using the wget command.
<pre style="color: silver; background: black;">wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/972/845/GCF_000972845.1_L_crocea_1.0/GCF_000972845.1_L_crocea_1.0_genomic.fna.gz
gunzip GCF_000972845.1_L_crocea_1.0_genomic.fna.gz</pre>
If you are feeling prudent, you can install the genomic, transcriptomic, and proteomic fastas (yes, all will be used in this tutorial, it is advised you download them now) with the command:
<pre style="color: silver; background: black;">sh -e genomic_and_protein_downloads</pre>
We will use the hisat2-build option to make a HISAT index file for the genome. It will create a set of files with the suffix .ht2, these files together build the index. What is an index and why is it helpful? Genome indexing is the same as indexing a tome, like an encyclopedia. It is much easier to locate information in the vastness of an encyclopedia when you consult the index, which is ordered in an easily navigatable way with pointers to the location of the information you seek within the encylopedia. Genome indexing is thus the structuring of a genome such that it is ordered in an easily navigatable way with pointers to where we can find whichever gene is being aligned. The genome index along with the trimmed fasta files are all you need to align the reads to the reference genome (the build command is included in the genome_indexing_and_alignment* files, so it is not necessary to run now).
<pre style="color: silver; background: black;">-bash-4.2$ module load hisat2
-bash-4.2$ hisat2-build
<strong>Usage: hisat2-build</strong> [options] <reference_in> <bt2_index_base>
reference_in                comma-separated list of files with ref sequences
hisat2_index_base           write ht2 data to files with this dir/basename

Options:
    -p                      number of threads</pre></strong>

-bash-4.2$ hisat2-build -p 4 GCF_000972845.1_L_crocea_1.0_genomic.fna L_crocea

After running the command, the following files will be generated as part of the index.  To refer to the index for  mapping the reads in the next step, you will use the file prefix, which in this case is: L_crocea
<pre style="color: silver; background: black;">-bash-4.2$ ls &#42;ht2
<strong>L_crocea.1.ht2
L_crocea.2.ht2
L_crocea.3.ht2
L_crocea.4.ht2
L_crocea.5.ht2
L_crocea.6.ht2
L_crocea.7.ht2
L_crocea.8.ht2</strong></pre>

Aligning the reads using HISAT2:<br>
Once we have created the index, the next step is to align the reads with HISAT2 using the index we created. The program will give the output in SAM format. We will not delve into the intricacies of the SAM format here, but it is recommended to peruse https://en.wikipedia.org/wiki/SAM_(file_format) again to garner a greater understanding. We align our reads with the following code:
<pre style="color: silver; background: black;">-bash-4.2$ module load hisat2
-bash-4.2$ hisat2
<strong>Usage</strong>: hisat2 [options]&#42; -x <ht2-idx>  [-S <sam>]
-x <ht2-idx>        path to the Index-filename-prefix (minus trailing .X.ht2) 

<strong>Options</strong>:
-q                  query input files are FASTQ .fq/.fastq (default)
-p                  number threads
--dta               reports alignments tailored for transcript assemblers

hisat2 -p 4 --dta -x ../index/L_crocea -q ../quality_control/trim_LB2A_SRR1964642.fastq -S trim_LB2A_SRR1964642.sam</pre>


The above must be repeated for all the files. You may run:
<pre style="color: silver; background: black;">-bash-4.2$ sbatch genome_indexing_and_alignment_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e genome_indexing_and_alignment_local</pre>

to process all four files appropriate for your setup.

Once the mapping have been completed, the file structure is as follows:
<pre style="color: silver; background: black;">-bash-4.2$ ls &#42;sam
<strong>trim_LB2A_SRR1964642.sam
trim_LB2A_SRR1964643.sam
trim_LC2A_SRR1964644.sam
trim_LC2A_SRR1964645.sam</strong></pre>

When HISAT2 completes its run, it will summarize each of it’s alignments, and it is written to the standard error file, which can be found in the same folder once the run is completed.

<pre style="color: silver; background: black;">-bash-4.2$ head -n 20 hisat2&#42;err
21799606 reads; of these:
  21799606 (100.00%) were unpaired; of these:
    1678851 (7.70%) aligned 0 times
    15828295 (72.61%) aligned exactly 1 time
    4292460 (19.69%) aligned >1 times
92.30% overall alignment rate</pre>

Let's have a look at the SAM file:

<pre style="color: silver; background: black;">-bash-4.2$ head trimmed_LB2A_SRR1964642.sam
@HD VN:1.0 SO:unsorted
@SQ SN:NW_017607850.1 LN:6737
@SQ SN:NW_017607851.1 LN:5396
@SQ SN:NW_017607852.1 LN:5050
@SQ SN:NW_017607853.1 LN:5873
@SQ SN:NW_017607854.1 LN:5692
@SQ SN:NW_017607855.1 LN:11509
@SQ SN:NW_017607856.1 LN:12722
@SQ SN:NW_017607857.1 LN:42555
@SQ SN:NW_017607858.1 LN:11917</pre>

After reading up on the SAM file format, you know that the "@" sign means that we are in the headings section, not the alignment section! The sam file is quite large so there is little purpose in scrolling to find the alignments section (the file is also much too large for using the "grep" command to locate the alignment section). Because of the density of the sam file, it is compressed to binary to create a more easily tractable file for manipulation by future programs. We convert the sam file to b<sub>inary</sub>am with the following command:

<pre style="color: silver; background: black;">-bash-4.2$ module load samtools
-bash-4.2$ samtools
<strong>Usage</strong>:   samtools <command> [options]

<strong>Commands</strong>:
  -- Indexing
     dict           create a sequence dictionary file
     faidx          index/extract FASTA
     index          index alignment

  -- Editing
     calmd          recalculate MD/NM tags and '=' bases
     fixmate        fix mate  Information
     reheader       replace BAM header
     targetcut      cut fosmid regions (for fosmid pool only)
     addreplacerg   adds or replaces RG tags
     markdup        mark duplicates

  -- File operations
     collate        shuffle and group alignments by name
     cat            concatenate BAMs
     merge          merge sorted alignments
     mpileup        multi-way pileup
     sort           sort alignment file
     split          splits a file by read group
     quickcheck     quickly check if SAM/BAM/CRAM file appears intact
     fastq          converts a BAM to a FASTQ
     fasta          converts a BAM to a FASTA

  -- Statistics
     bedcov         read depth per BED region
     depth          compute the depth
     flagstat       simple stats
     idxstats       BAM index stats
     phase          phase heterozygotes
     stats          generate stats (former bamcheck)

  -- Viewing
     flags          explain BAM flags
     tview          text alignment viewer
     view           SAM<->BAM<->CRAM conversion
     depad          convert padded BAM to unpadded BAM
</pre>

We are truly only interested in sorting our SAM files.

<pre style="color: silver; background: black;">-bash-4.2$ samtools sort

<strong>Usage</strong>: samtools sort [options...] [in.bam]
<strong>Options</strong>:
  -l INT     Set compression level, from 0 (uncompressed) to 9 (best)
  -m INT     Set maximum memory per thread; suffix K/M/G recognized [768M]
  -n         Sort by read name
  -t TAG     Sort by value of TAG. Uses position as secondary index (or read name if -n is set)
  -o FILE    Write final output to FILE rather than standard output
  -T PREFIX  Write temporary files to PREFIX.nnnn.bam
      --input-fmt-option OPT[=VAL]
               Specify a single input file format option in the form
               of OPTION or OPTION=VALUE
  -O, --output-fmt FORMAT[,OPT[=VAL]]...
               Specify output format (SAM, BAM, CRAM)
      --output-fmt-option OPT[=VAL]
               Specify a single output file format option in the form
               of OPTION or OPTION=VALUE
      --reference FILE
               Reference sequence FASTA FILE [null]
  -@, --threads INT
               Number of additional threads to use [0]
</pre>

The sort function converts SAM files to BAM automatically. Therefore, we can cut through most of these options and do a simple "samtools sort -o <output.bam> <inupt.sam>. Let's write our command:
<pre style="color: silver; background: black;">-bash-4.2$ samtools sort -@ 4 -o sort_trim_LB2A_SRR1964642.bam trimmed_LB2A_SRR1964642.sam</pre>

All samples may be run by executing the following command:
<pre style="color: silver; background: black;">-bash-4.2$ sbatch sam_to_bam_xanadu.sh</pre>
or
<pre style="color: silver; background: black;">sh -e sam_to_bam_local</pre>
appropriate for your set-up.

Once the conversion is done you will have the following files in the directory.
<pre style="color: silver; background: black;">-bash-4.2$ ls sort&#42;
<strong>sort_trim_LB2A_SRR1964642.bam
sort_trim_LB2A_SRR1964643.bam
sort_trim_LC2A_SRR1964644.bam
sort_trim_LC2A_SRR1964645.bam</strong></pre>

<h2 id="Fifth_Point_Header">Generating total read counts from alignent using htseq-count</h2>
Now we will be using the <a href="http://htseq.readthedocs.io/en/master/count.htmhtseq-count">htseq-count</a> function of the module htseq to count the reads which have mapped to the genome. The thought behind htseq-count is quite intuitive, enumerating matching alignments into a "counts" file. However, this belies the complexity of alignment counting. htseq is actually a Python module. Because of this, you'll notice something strange if you load it into the submit node:
<pre style="color: silver; background: black;">-bash-4.2$ module load htseq
(0.9.1) -bash-4.2$</pre>

We are "logged in" to a new shell! Let's try viewing the help menu:
<pre style="color: silver; background: black;">-bash-4.2$ module load htseq
(0.9.1) -bash-4.2$ htseq
<strong>-bash: htseq: command not found</strong>
</pre>

Let's try viewing the help menu for htseq-count:
<pre style="color: silver; background: black;">-bash-4.2$ module load htseq
(0.9.1) -bash-4.2$ htseq-count</pre>

Your terminal should become hung up on whitespace. Because hsteq operates as a shell (within our shell), if we try to run the command with no input then it gets stuck in an infinite loop! Press CTRL + C to kill the infinite loop and unload the module to acccess your normal shell:

<pre style="color: silver; background: black;">-bash-4.2$ module unload htseq</pre>

Because of this behavior, you will have to trust me on our choice of options. We run htseq count with the following commands:

<pre style="color: silver; background: black;">-bash-4.2$ module load htseq
-bash-4.2$ htseq-count -s no -r pos -t gene -i Dbxref -f bam ../mapping/sort_trim_LB2A_SRR1964642.bam GCF_000972845.1_L_crocea_1.0_genomic.gff > LB2A_SRR1964642.counts</pre>


<pre style="color: silver; background: black;"><strong>Usage</strong>: htseq-count [options] alignment_file gff_file
<strong>Options</strong>:
  -f SAMTYPE, --format=SAMTYPE
                        type of  data, either 'sam' or 'bam'
                        (default: sam)
  -r ORDER, --order=ORDER
                        'pos' or 'name'. Sorting order of
                        (default: name). Paired-end sequencing data must be
                        sorted either by position or by read name, and the
                        sorting order must be specified. Ignored for single-
                        end data.
  -s STRANDED, --stranded=STRANDED
                        whether the data is from a strand-specific assay.
                        Specify 'yes', 'no', or 'reverse' (default: yes).
                        'reverse' means 'yes' with reversed strand
                        interpretation
  -t FEATURETYPE, --type=FEATURETYPE
                        feature type (3rd column in GFF file) to be used, all
                        features of other type are ignored (default, suitable
                        for Ensembl GTF files: exon)
  -i IDATTR, --idattr=IDATTR
                        GFF attribute to be used as feature ID (default,
                        suitable for Ensembl GTF files: gene_id)</pre>

 
The above command should be repeated for all other BAM files as well. You can process all the BAM files locally with the command:
<pre style="color: silver; background: black;">sh -e htseq_count</pre>
or on Xanadu with (do not forget to nano the script to insert your email):
<pre style="color: silver; background: black;">-bash-4.2$ sbatch htseq_count.sh</pre>
Once all the bam files have been counted, we will be having the following files in the directory.<br>
<pre style="color: silver; background: black;">-bash-4.2$ ls &#42;counts
<strong>sort_trim_LB2A_SRR1964642.counts
sort_trim_LB2A_SRR1964643.counts
sort_trim_LC2A_SRR1964644.counts
sort_trim_LC2A_SRR1964645.counts</strong></pre>

Let's have a look at the contents of a counts file:
<pre style="color: silver; background: black;">-bash-4.2$ head sort_trim_LB2A_SRR1964642.counts
GeneID:104917625	18
GeneID:104917626	7
GeneID:104917627	0
GeneID:104917628	199
GeneID:104917629	71
GeneID:104917630	23
GeneID:104917631	111
GeneID:104917632	25
GeneID:104917634	276
GeneID:104917635	254</pre>

We see the layout is quite straightforward, with two columns separated by a tab. The first column identifies the gene from the eponymous sample and the second column is the number of mRNA strands from the row's gene found in the sample. This setup is perfect for our next task, identifying differentially expressed genes.



<h2 id="Sixth_Point_Header">Pairwise differential expression with counts in R using DESeq2</h2>
This part of the tutorial <i>must</i> be run locally. To download the appropriate files to your local computer, we will use the secure copy client, "scp". Close your Xanadu connection and run the following code:
<pre style="color: silver; background: black;">-bash-4.2$ exit
<strong>logout
Connection to xanadu-submit-ext.cam.uchc.edu closed.</strong>
user@user$ scp your.user.name@xanadu-submit-ext.cam.uchc.edu:/path/to/counts/&#42;.counts /path/to/local/destination</pre>
Voila! Piece of cake.

<h3 id="types_of_plots">Common plots for differential expression analysis</h3>

To identify differentially expressed genes, We will use the DESeq2 package within Bioconductor in R to perform normalization and statistical analysis of differences among our two sample groups. This R-code is executed in RStudio for R version 3.4.3 (if the r_installation file did not properly install R 3.4.3 you may visit https://linode.com/docs/development/r/how-to-install-r-on-ubuntu-and-debian/ to troubleshoot). Note that Bioconductor will not run on any previous version of R in Linux, so it is imperative that you successfully install R 3.4.3). For our differential expression analysis, we will be using three types of graphs to visualize the data: Bland-Altman (MA), heatmap, and PCA plots. Let's review each plot before diving in:

<b>Bland-Altman(MA) Plot</b><br>
The Bland-Altman plot is a binary analysis, gathering information on the relationships between two-sets of data. Because we have four sets of data, we will be breaking our data into two separate groups (LB2A and LC2A), then generating two superimposed plots comparing each member of the two groups to each member of the other group. Before we go further, what <i>is</i> a Bland-Altman plot? An MA plot is the comparison of the log-differences for two datasets to the log-averages. That is:

<pre {
	font-family: Cambria,Georgia,serif;
	font-size: 13px;
	font-style: normal;
	font-variant: normal;
	font-weight: 400;
	line-height: 18.57px;
}><strong><i>M</i> = log<sub>2</sub>(D<sub>1</sub>/D<sub>2</sub>) = log<sub>2</sub>(D<sub>1</sub>)-log<sub>2</sub>(D<sub>2</sub>)
<i>A</i> = &frac12;log<sub>2</sub>(D<sub>1</sub>D<sub>2</sub>) = &frac12;(log<sub>2</sub>(D<sub>1</sub>)+log<sub>2</sub>(D<sub>2</sub>))</strong></pre>
with D<sub>1</sub> and D<sub>2</sub> being datasets 1 and 2, respectively. We plot M on the vertical axis and A on the horizontal axis. We see here that should two datapoints from datasets 1 and 2 be equal, they will plot at (D<sub>1</sub>==D<sub>2</sub>, 0). Therefore, should our entire plot run along y=0, it is safe to say, that for us, none of our genes were differentially expressed. However, should there be large deviations from y=0, we are provided with insight that we may have differentially expressed genes.

<b>Heatmap</b><br>
Heatmaps are the most readily readable visualizations for determining differential expression, unless, like me, you are colorblind! Heatmaps are discrete plots. That is, the values along the x and y axis move in integer increments, with no places in-between. Typically, the x-and-y axes contain different types of information about each dataset, and combine to capture the whole image of the dataset. For us, we may find it useful to make one axis our groups (our four datasets) and the other axis the genes sampled from those datasets. Now each cell in our grid will correspond to the expression of a gene from a sample. We then assign a color-palette to each possible range of gene expression. For instance, an expression between 300-500 may be blue while an expression of 501-1000 may be violet. After coloring all of our cells responsibly, we now have simple, discrete snapshots of each datasets expression distribution. Should we notice that two groups have very similar looking snapshots, we may cluster those groups safely with the assumption that they do not share differentially expressed genes, and therefore belong to the same space. 

<b>Principal Component Analysis(PCA)</b><br>
Principal-Component-Analysis is far too complex and nuanced to go into detail here. However, you may download an entire tutorial <a href="https://docs.wixstatic.com/ugd/d07f4e_eafc861e1cff411e88b89cf2739e8565.pdf">here</a> for differential expression analysis in R which covers much of what we are about to do, including Principal Component Analysis (pages 15-18, "singular-value-decomposition"). Now! Let's get coding!

<h3 id="using_deseq2">Using DESeq2</h2>

The DESeq2 vignette comes with complete instructions on how to use DESeq2 for analysis. If you are ever confused, visit the vignette, find the appropriate step and read up. This is true for most Bioconductor packages, so do not be afraid to try new software! The vignette is your best friend. This tutorial includes instruction only on using DESeq2, but if you are curious there is a <a href="https://github.com/wolf-adam-eily/how_deseq2_works">separate tutorial</a> which details the exact mathematical and statistical procedure for every step of the DESeq2 software. When you have time, I encourage you to read through and understand the material in the supplemental lecture. While the math and statistics are of an advanced level, understanding them will provide you with immeasurable intution for solving non-traditional bioinformatical problems which may pop up in your research. Now, let's begin loading our data:
<pre style="color: silver; background: black;">
library("DESeq2")
# Set the working directory
directory <- "~/your_directory_with_htseq_counts"
setwd(directory)
list.files(directory)
</pre>
We will be generating a variety of files. We want the names of the files to inform us of exactly what information is contained in each file. The data is from the Croaker dataset, and we are analyzing the data with DESeq2. We choose an "outputPrefix" which, as the name suggests, will prefix all of our files. Secondly, we want easy loading of our files. We can do this by creating a string vector with each element being a counts filename. The easiest way to ensure you've typed the name right is to type the first few letters of the filename and press TAB. You should be presented with an option of names to auto-fill. Press up or down to toggle your choice and lastly press ENTER to auto-fill. If your files do not come up, then you are in the wrong directory!

<pre style="color: silver; background: black;">
outputPrefix <- "Croaker_DESeq2"
sampleFiles<- c("sort_trim_LB2A_SRR1964642.counts","sort_trim_LB2A_SRR1964643.counts",
                "sort_trim_LC2A_SRR1964644.counts", "sort_trim_LC2A_SRR1964645.counts")
&num; Liver mRNA profiles of control group: (LB2A) 
&num; Liver mRNA profiles of thermal stress group: (LC2A)
&num; ""CONTROL"" LB2A_1: sort_trim_LB2A_SRR1964642.counts, LB2A_2: sort_trim_LB2A_SRR1964643.counts
&num; ""TREATED"" LC2A_1: sort_trim_LB2A_SRR1964644.counts, LC2A_2: sort_trim_LC2A_SRR1964645.counts</pre>

Always be as specific as possible in your comments. Most often your code is published as supplemental information for your research. If a scientist is reading your code and is unsure of exactly what you are doing it harms your reproducibility. Moreover, it harms your credibility! The best coding leaves nothing to the imagination. Before we move any further, let's find some instructions on how to use DESeq2. This can be done typing in ??'DESeq2-pavkage' and pressing enter. You should see the following vignette:
<pre style="color: silver; background: black;">
DESeq {DESeq2}	R Documentation
Differential expression analysis based on the Negative Binomial (a.k.a. Gamma-Poisson) distribution

<strong>Description</strong>

<em>This function performs a default analysis through the steps:

estimation of size factors: <u>estimateSizeFactors</u>

estimation of dispersion: <u>estimateDispersions</u>

Negative Binomial GLM fitting and Wald statistics: <u>nbinomWaldTest</u>

For complete details on each step, see the manual pages of the respective functions. After the DESeq function returns a DESeqDataSet 
object, results tables (log2 fold changes and p-values) can be generated using the results function. Shrunken LFC can then be 
generated using the lfcShrink function. All support questions should be posted to the Bioconductor support site: 
http://support.bioconductor.org.</em>

<strong>Usage</strong>

DESeq(object, test = c("Wald", "LRT"), fitType = c("parametric", "local",
  "mean"), sfType = c("ratio", "poscounts", "iterate"), betaPrior,
  full = design(object), reduced, quiet = FALSE,
  minReplicatesForReplace = 7, modelMatrixType, useT = FALSE, minmu = 0.5,
  parallel = FALSE, BPPARAM = bpparam())
<strong>Arguments</strong>

object		a DESeqDataSet object, see the constructor functions DESeqDataSet, DESeqDataSetFromMatrix, 
		<u>DESeqDataSetFromHTSeqCount</u>.
test		either "Wald" or "LRT", which will then use either Wald significance tests (defined by nbinomWaldTest), or the 
		likelihood ratio test on the difference in deviance between a full and reduced model formula (defined by nbinomLRT)
fitType		either "parametric", "local", or "mean" for the type of fitting of dispersions to the mean intensity. See 
		estimateDispersions for description.
sfType		either "ratio", "poscounts", or "iterate" for teh type of size factor estimation. See estimateSizeFactors for 
		description.
betaPrior	whether or not to put a zero-mean normal prior on the non-intercept coefficients See nbinomWaldTest for description of 
		the calculation of the beta prior. In versions >=1.16, the default is set to FALSE, and shrunken LFCs are obtained 
		afterwards using lfcShrink.
full		for test="LRT", the full model formula, which is restricted to the formula in design(object). alternatively, it can be 
		a model matrix constructed by the user. advanced use: specifying a model matrix for full and test="Wald" is possible 
		if betaPrior=FALSE
reduced		for test="LRT", a reduced formula to compare against, i.e., the full formula with the term(s) of interest removed. 
		alternatively, it can be a model matrix constructed by the user
quiet		whether to print messages at each step
minReplicatesForReplace	the minimum number of replicates required in order to use replaceOutliers on a sample. If there are samples 
			with so many replicates, the model will be refit after these replacing outliers, flagged by Cook's distance. 
			Set to Inf in order to never replace outliers.
modelMatrixType		either "standard" or "expanded", which describe how the model matrix, X of the GLM formula is formed. 
			"standard" is as created by model.matrix using the design formula. "expanded" includes an indicator variable 
			for each level of factors in addition to an intercept. for more information see the Description of 
			nbinomWaldTest. betaPrior must be set to TRUE in order for expanded model matrices to be fit.
useT		logical, passed to nbinomWaldTest, default is FALSE, where Wald statistics are assumed to follow a standard Normal
minmu		lower bound on the estimated count for fitting gene-wise dispersion and for use with nbinomWaldTest and nbinomLRT
parallel	if FALSE, no parallelization. if TRUE, parallel execution using BiocParallel, see next argument BPPARAM. A note on 
		running in parallel using BiocParallel: it may be advantageous to remove large, unneeded objects from your current R 
		environment before calling DESeq, as it is possible that R's internal garbage collection will copy these files while 
		running on worker nodes.
BPPARAM		an optional parameter object passed internally to bplapply when parallel=TRUE. If not specified, the parameters last 
		registered with register will be used.</pre>
		

The vignette informs us that DESeq2 performs three main steps in its differential expression analysis: estimating size factors, estimating dispersion, and conducting the Wald Test. These steps are completed so as to fit a <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial distribution</a>. Let's review these terms, first:<br>
<br>
Estimating size factors: This step operates under the simple assumption that if a sample is up-regulated then its transcriptome is larger than those which are not up-regulated. Suppose we have two samples, A and B. Every gene in A has been up-regulated by a factor of 2 and B is unchanged. If we took the proportion of gene expression in both samples they would return the same value! While A is twice as expressed, its transcriptome is twice as large. Our analysis would then conclude that no genes were differentially expressed, even though they all were. Estimating size factors is a more succinct way of stating that DESeq2 finds an idealized transcriptome size for all of the samples. Now all of the individual gene count proportions are calculating using the idealized transcriptome size and not the individual sample size. Because the gene counts themselves are not changed, our analysis will now return that each gene is differentially expressed with a <a href="https://en.wikipedia.org/wiki/Fold_change">fold change of two</a>.<br>
	
Estimating Dispersion: Dispersion is another term for variance, or the distance we expect a random gene count to be away from the average gene count. While the negative binomial distribution is modeled with the parameters of probability of success, number of successes, and number of failures, we cannot create our distribution with these values. . . because we do not know them! Furthermore, we cannot calculate the number of failures because if a read failed to map then it isn't present in the counts file. We can get around this hurdle by reparameterizing the negative binomial distribution in terms of the mean and variance. Our size factor from before is technically our mean, which leaves us with having to determine the variance. Through a series of mathematical and statistical manipulations, DESeq2 calculates the idealized variance of our samples.

Wald Test: The Wald Test is a statistical test which can be used to assess the confidence in the results. After we finish creating our experimental distributions, the value of each gene in our experimental distribution is compared to its hypothetical value in the model distribution. By taking the ratio we get the percent similarity. Lastly, subtracting our percent similarity from 1 will give us our percent dissimilarity. This is the value DESeq2 returns as our <a href="https://en.wikipedia.org/wiki/P-value">p-value</a>. You can think of this value as the percentage of chance that we're wrong about the fold change of a differentially expressed gene. Typically, we take all genes with p-values below 0.05. This is not the true <a href="https://en.wikipedia.org/wiki/Wald_test">Wald Test</a>, but is the purpose behind the true statistic. 

Furthermore, DESeq2 informs us that clicking on any individual step will present us with instructions. Our first step is estimating our size factors. Let's click on the link and see this vignette:

<pre style="color: silver; background: black;">
estimateSizeFactors {DESeq2}	R Documentation
Estimate the size factors for a DESeqDataSet

<strong>Description</strong>

<em>This function estimates the size factors using the "median ratio method" described by Equation 5 in Anders and Huber (2010). The 
estimated size factors can be accessed using the accessor function sizeFactors. Alternative library size estimators can also be 
supplied using the assignment function sizeFactors<-.</em>

<strong>Usage</strong>

## S4 method for signature 'DESeqDataSet'
estimateSizeFactors(object, type = c("ratio",
  "poscounts", "iterate"), locfunc = stats::median, geoMeans, controlGenes,
  normMatrix)
<strong>Arguments</strong>

object		a DESeqDataSet
type		Method for estimation: either "ratio", "poscounts", or "iterate". "ratio" uses the standard median ratio method 
		introduced in DESeq. The size factor is the median ratio of the sample over a "pseudosample": for each gene, the 
		geometric mean of all samples. "poscounts" and "iterate" offer alternative estimators, which can be used even when all 
		genes contain a sample with a zero (a problem for the default method, as the geometric mean becomes zero, and the 
		ratio undefined). The "poscounts" estimator deals with a gene with some zeros, by calculating a modified geometric 
		mean by taking the n-th root of the product of the non-zero counts. This evolved out of use cases with Paul McMurdie's 
		phyloseq package for metagenomic samples. The "iterate" estimator iterates between estimating the dispersion with a 
		design of ~1, and finding a size factor vector by numerically optimizing the likelihood of the ~1 model.
locfunc		a function to compute a location for a sample. By default, the median is used. However, especially for low counts, the 
		shorth function from the genefilter package may give better results.
geoMeans	by default this is not provided and the geometric means of the counts are calculated within the function. A vector of 
		geometric means from another count matrix can be provided for a "frozen" size factor calculation
controlGenes	optional, numeric or logical index vector specifying those genes to use for size factor estimation (e.g. housekeeping 
		or spike-in genes)
normMatrix	optional, a matrix of normalization factors which do not yet control for library size. Note that this argument should 
		not be used (and will be ignored) if the dds object was created using tximport. In this case, the information in 
		assays(dds)[["avgTxLength"]] is automatically used to create appropriate normalization factors. Providing normMatrix 
		will estimate size factors on the count matrix divided by normMatrix and store the product of the size factors and 
		normMatrix as normalizationFactors. It is recommended to divide out the row-wise geometric mean of normMatrix so the 
		rows roughly are centered on 1.
<strong>Details</strong>

Typically, the function is called with the idiom:

dds <- estimateSizeFactors(dds)</pre>

We see that the only argument we need is a DESeq dataset. Let's create ours. From looking at the homepage of the DESeq2 vignete, we see that the "object" argument comes with links for a variety of data sources. Our data is from htseq-count, so we click on "DESeqDataSetFromHTSeqCount" in object row of the arguments. We see this vignette:

<pre style="color: silver; background: black;">
DESeqDataSet-class {DESeq2}	R Documentation
DESeqDataSet object and constructors

<strong>Description</strong>

<em>DESeqDataSet is a subclass of RangedSummarizedExperiment, used to store the input values, intermediate calculations and results of an analysis of differential expression. The DESeqDataSet class enforces non-negative integer values in the "counts" matrix stored as the first element in the assay list. In addition, a formula which specifies the design of the experiment must be provided. The constructor functions create a DESeqDataSet object from various types of input: a RangedSummarizedExperiment, a matrix, count files generated by the python package HTSeq, or a list from the tximport function in the tximport package. See the vignette for examples of construction from different types.</em>

<strong>Usage</strong>

DESeqDataSet(se, design, ignoreRank = FALSE)

DESeqDataSetFromMatrix(countData, colData, design, tidy = FALSE,
  ignoreRank = FALSE, ...)

DESeqDataSetFromHTSeqCount(sampleTable, directory = ".", design,
  ignoreRank = FALSE, ...)

DESeqDataSetFromTximport(txi, colData, design, ...)
<strong>Arguments</strong>

se		a RangedSummarizedExperiment with columns of variables indicating sample information in colData, and the counts as the 
		first element in the assays list, which will be renamed "counts". A RangedSummarizedExperiment object can be generated 
		by the function summarizeOverlaps in the GenomicAlignments package.
design		a formula or matrix. the formula expresses how the counts for each gene depend on the variables in colData. Many R 
		formula are valid, including designs with multiple variables, e.g., ~ group + condition, and designs with 
		interactions, e.g., ~ genotype + treatment + genotype:treatment. See results for a variety of designs and how to 
		extract results tables. By default, the functions in this package will use the last variable in the formula for 
		building results tables and plotting. ~ 1 can be used for no design, although users need to remember to switch to 
		another design for differential testing.
ignoreRank	use of this argument is reserved for DEXSeq developers only. Users will immediately encounter an error upon trying to 
		estimate dispersion using a design with a model matrix which is not full rank.
countData	for matrix input: a matrix of non-negative integers
colData		for matrix input: a DataFrame or data.frame with at least a single column. Rows of colData correspond to columns of 
		countData
tidy		for matrix input: whether the first column of countData is the rownames for the count matrix
...	
arguments provided to SummarizedExperiment including rowRanges and metadata. Note that for Bioconductor 3.1, rowRanges must be a 
GRanges or GRangesList, with potential metadata columns as a DataFrame accessed and stored with mcols. If a user wants to store 
metadata columns about the rows of the countData, but does not have GRanges or GRangesList information, first construct the 
DESeqDataSet without rowRanges and then add the DataFrame with mcols(dds).
sampleTable	for htseq-count: a data.frame with three or more columns. Each row describes one sample. The first column is the s
		sample name, the second column the file name of the count file generated by htseq-count, and the remaining columns are 
		sample metadata which will be stored in colData
directory	for htseq-count: the directory relative to which the filenames are specified. defaults to current directory
txi		for tximport: the simple list output of the tximport function
<strong>Details</strong>

Note on the error message "assay colnames() must be NULL or equal colData rownames()": this means that the colnames of countData are different than the rownames of colData. Fix this with: colnames(countData) <- NULL

<strong>Value</strong>
A DESeqDataSet object.</pre>

We follow these instructions exactly to create our DESeq object:

<pre style="color: silver; background: black;">
sampleNames <- c("LB2A_1","LB2A_2","LC2A_1","LC2A_2")
sampleCondition <- c("control","control","treated","treated")
</pre>

Let's create a dataframe. The options we are using for the data.frame function are a part of <a href="https://bioconductor.org/packages/release/bioc/html/Biobase.html">Biobase</a>. You can find out about each argument by typing in the argument name and pressing TAB. For instance, "data.frame(sampleName(TAB) "

<pre style="color: silver; background: black;">
sampleTable <- data.frame(sampleName = sampleNames,
                          fileName = sampleFiles,
                          condition = sampleCondition)
			  
ddsHTSeq <- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable,
                                       directory = directory,
                                       design = ~ condition)</pre>
				       
				       
After creating our DESeq object, we go back to the DESeq function vignette. Very pleasantly, we see that it is as easy as typing "DESeq(object)". Afterwards, the vignette informs us that "results(dds)" will calculate the complete results, including p-values, fold changes, and more. Let's do this:

<pre style="color: silver; background: black;">
&num; Differential expression analysis
&num;differential expression analysis steps are wrapped into a single function, DESeq()
dds <- DESeq(ddsHTSeq)
<strong>estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing</strong>
 
res <- results(dds)
head(res)
<strong>
DataFrame with 6 rows and 6 columns
log2 fold change (MLE): condition treated vs control 
Wald test p-value: condition treated vs control 
DataFrame with 6 rows and 6 columns
                         baseMean     log2FoldChange             lfcSE               stat              pvalue                padj
                        <numeric>          <numeric>         <numeric>          <numeric>           <numeric>           <numeric>
GeneID:104917625 25.1788677883808 -0.351267493371916  0.74406185433732 -0.472094478872007   0.636859353157054   0.791446951187614
GeneID:104917626 10.5869934203484   1.24302237624161 0.945050102805392   1.31529785833754   0.188409817629488   0.366925219368594
GeneID:104917627                0                 NA                NA                 NA                  NA                  NA
GeneID:104917628 220.020215686258  0.431984191910497 0.207205565359478   2.08480979341001  0.0370865693995318   0.110551743788831
GeneID:104917629 64.9709160307111  -0.12477576210606 0.375096700983498  -0.33264958550395    0.73939880906362   0.856594480025376
GeneID:104917630 11.9088308244003  -3.85266236717162  1.17626616301573   -3.2753321385137 0.00105537813187126 0.00551098076212648</strong></pre>

DESeq takes our counts and applies the log-base-2 to them. Therefore, the true expression mean of the first gene is 2<sup>25.18</sup>, the second 2<sup>10.59</sup>, and so on. We see in column two, log2FoldChange, that the first gene was down-regulated going from the control group to the treated group, the second was up-regulated from the control group to the treated group. This information is directional. Whatever order your samples are inserted into your dataframe will determine the direction of the fold change. Should we have placed the treated group first our fold change calculation in the results table will have the opposite sign for each gene. The third column, lfcSE (log-fold change standard error), is a value of how the experimental log fold change value deviated from the idealized model. For genes with very large log-fold changes there is more leniancy in a larger standard error. This is because the standard error is the difference betweeen the experimental and idealized log-fold changes <i>squared</i>. Next, we have the "stat" column, which is the log-fold-2-change column divided by the log-fold-change-standard error column. The sign of the "stat" value does not matter, but its value does. The larger the "stat" value, the more confident we can be in the differential expression of the gene. You can think about the reason for this on your own. Lastly, we have our p-values, which we have discussed earlier, and our adjusted p-value. You needn't worry too much about how the adjusted p-value is calculated, as there are multiple ways. Just know that the adjusted p value is the better predictor of accuracy than the normal p-value. Now, let's look at a summary of our results and write them to a table. Remember, we want to be 95% certain (p-value < 0.05) that our genes are differentially expressed:

<pre style="color: silver; background: black;">
summary(res)
<strong>out of 20227 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)       : 2862, 14%
LFC < 0 (down)     : 2624, 13%
outliers [1]       : 0, 0%
low counts [2]     : 3465, 17%
(mean count < 2)
[1] see 'cooksCutoff' argument of ?results
	[2] see 'independentFiltering' argument of ?results</strong>


&num; retrieve our genes in which we're more than 95% certain
dim(res)
<strong>[1] 27239     6</strong>
res = subset(res, padj<0.05)
dim(res)
<strong>[1] 4628    6</strong>
&num; we have removed many genes!
&num; Let's order our results by padj value (most confident to least confident)
res <- res[order(res$padj),]

&num; we use the "merge" function to combine data frames

&num; we can access the raw counts information using the DESeq function "counts(DESeq object)"

#num; let's create a data frame which has all of our statistics and the raw counts
resdata <- merge(as.data.frame(res), 
                 as.data.frame(counts(dds,normalized =TRUE)), 
                 by = 'row.names', sort = FALSE)
names(resdata)[1] <- 'gene'

&num; now let's write this data frame to a csv

write.csv(resdata, file = paste0(outputPrefix, "-results-with-normalized.csv"))</pre><br>
Our -results-with-normalized file contains all of the information of our -replaceoutliers-results and additionally the normalized counts of each sample. Let's have a look at it in the terminal:

<pre style="color: silver; background: black;">head Croaker_DESeq2-results-with-normalized.csv
"","gene","baseMean","log2FoldChange","lfcSE","stat","pvalue","padj","LB2A_1","LB2A_2","LC2A_1","LC2A_2"
"1","GeneID:104917796",4769.95035549771,3.26790212928836,0.0653853095785126,49.9791489916304,0,0,901.106965996412,893.344158166973,8574.41533538478,8710.93496244265
"2","GeneID:104918527",15829.3714065108,1.9381028852635,0.0442492786239224,43.7996492945244,0,0,6560.70781492939,6543.00968591525,25098.6197234442,25115.1484017544
"3","GeneID:104920815",10289.5933862363,1.9899626227926,0.0490973726004825,40.5309391805019,0,0,4107.48228250271,4170.24833394868,16337.3327233147,16543.310205179
"4","GeneID:104921101",3409.31956503535,-4.91775690540935,0.0946450825092183,-51.9599832873553,0,0,6572.16256449714,6628.41731422352,215.779988572597,220.918392848141
"5","GeneID:104921941",4734.72693038149,2.76662808733791,0.0624881461008418,44.27444659461,0,0,1221.83995389344,1204.54206820975,8288.42941751584,8224.09628190694
"6","GeneID:104924020",12853.7020528709,-1.88815134014643,0.047291546513469,-39.9257685431935,0,0,20030.5387441406,20448.7459501297,5466.7705238847,5468.75299332874
"7","GeneID:104924701",2370.03445131097,-3.76035093454651,0.0920739957113332,-40.8405316343152,0,0,4482.62533084656,4345.97207495076,331.413283884228,320.127115562352
"8","GeneID:104925605",11058.8260524593,5.56728188277741,0.0676448603229138,82.3016243392489,0,0,463.917357493916,449.617169714806,21652.3345471029,21669.4351355257
"9","GeneID:104927028",2535.71264215129,-3.30668469336064,0.0834467995056178,-39.6262614378401,0,0,4570.44507753265,4641.46283495983,460.468300973102,470.474355139559</pre>
R
<pre style="color: silver; background: black;">&num; Let's compile all of our normalized counts to tab delimited file for future analyses
write.table(as.data.frame(counts(dds),normalized=T), 
            file = paste0(outputPrefix, "_normalized_counts.txt"), sep = '\t')

&num; Let's produce a data frame with descriptions of our calculated values

&num; 'mcols' retrieves the metadata associated with each column. 

&num; you may think of metadata as simply a description of the value-type 

mcols(res, use.names = T)
<strong>DataFrame with 6 rows and 2 columns
                       type                                          description
                <character>                                          <character>
baseMean       intermediate            mean of normalized counts for all samples
log2FoldChange      results log2 fold change (MLE): condition treated vs control
lfcSE               results         standard error: condition treated vs control
stat                results         Wald statistic: condition treated vs control
pvalue              results      Wald test p-value: condition treated vs control
			padj                results                                 BH adjusted p-values</strong>

write.csv(as.data.frame(mcols(res, use.name = T)),
          file = paste0(outputPrefix, "-test-conditions.csv"))</pre><br>
Our test-conditions file gives us insight into what statistics were calculated and how those statistics were calculated. Let's look at it in the terminal:
<pre style="color: silver; background: black;">head Croaker_DESeq2-test-conditions.csv
"","type","description"
"baseMean","intermediate","mean of normalized counts for all samples"
"log2FoldChange","results","log2 fold change (MLE): condition treated vs control"
"lfcSE","results","standard error: condition treated vs control"
"stat","results","Wald statistic: condition treated vs control"
"pvalue","results","Wald test p-value: condition treated vs control"
"padj","results","BH adjusted p-values</pre>

We can clean up our data by replacing outliers with the value predicted from the idealized distribution. This can only be done if you have multiple replicates per condition! DESeq2 will automatically do this if you have 7 or more replicates. We use the "trimmed mean" approach. the "trimmed mean" approach reduces the smallest and largest values in a row by a small percentage, calculates the mean, and
then finds the "trimmed mean" on the idealized distribution and replaces the outlier with the idealized value of the trimmed mean. The replace outliers function can only be used on DESeqData objects

<pre style="color: silver; background:black;">
ddsClean <- replaceOutliersWithTrimmedMean(dds)

&num; now let's perform our differential expression analysis with the outliers replaced

ddsClean <- DESeq(ddsClean)
<strong>using pre-existing size factors
estimating dispersions
found already estimated dispersions, replacing these
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing</strong>

temp_ddsClean <- ddsClean

&num; let's compare our cleaned results to our normal results

initial = results(dds)$padj <0.05
head(initial)
<strong>[1] FALSE FALSE    NA FALSE FALSE  TRUE</strong>

length(initial)==length(cleaned)
<strong<[1] TRUE</strong>

&num; we need to remove the NAs

keep_vector_initial = is.na(initial)==FALSE
keep_vector_cleaned = is.na(cleaned)==FALSE
identical(keep_vector_initial,keep_vector_cleaned)
<strong>[1] TRUE</strong>

initial = initial[keep_vector_initial]
cleaned = cleaned[keep_vector_cleaned]
length(initial)==length(cleaned)
<strong>[1] TRUE</strong>

&num; now let's determine how many of these have the same value

sum(initial==cleaned)
<strong>[1] 16762</strong>

&num; we see that 16762 of our genes have p-values below 0.05 in both our cleaned and normal samples

&num; let's write a file which contains our significant genes from our cleaned samples

resClean <- results(ddsClean)
resClean = subset(res, padj<0.05)
resClean <- resClean[order(resClean$padj),]
write.csv(as.data.frame(resClean),file = paste0(outputPrefix, "-replaceoutliers-results.csv"))</pre><br>

Let's have a look at this file in the terminal:

<pre style="color: silver; background: black;"> head Croaker_DESeq2-replaceoutliers-results.csv
"","baseMean","log2FoldChange","lfcSE","stat","pvalue","padj"
"GeneID:104917796",4769.95035549771,3.26790212928836,0.0653853095785126,49.9791489916304,0,0
"GeneID:104918527",15829.3714065108,1.9381028852635,0.0442492786239224,43.7996492945244,0,0
"GeneID:104920815",10289.5933862363,1.9899626227926,0.0490973726004825,40.5309391805019,0,0
"GeneID:104921101",3409.31956503535,-4.91775690540935,0.0946450825092183,-51.9599832873553,0,0
"GeneID:104921941",4734.72693038149,2.76662808733791,0.0624881461008418,44.27444659461,0,0
"GeneID:104924020",12853.7020528709,-1.88815134014643,0.047291546513469,-39.9257685431935,0,0
"GeneID:104924701",2370.03445131097,-3.76035093454651,0.0920739957113332,-40.8405316343152,0,0
"GeneID:104925605",11058.8260524593,5.56728188277741,0.0676448603229138,82.3016243392489,0,0
"GeneID:104927028",2535.71264215129,-3.30668469336064,0.0834467995056178,-39.6262614378401,0,0</pre>

From our previous file of this type we are well aware of the meaning of each column. It is always wise to keep files for analyses of each transformed dataset. The first reason for this is that bioinformatics is no different than a wet-lab environment: you must keep detailed notes of exactly what you did and when if you want to ensure reproducibility. The second reason is that you may find that a specific trasnformation which is meant to reduce statistical noise actually obfuscates your findings! If you did not have a record of the datasets before transformation you would have to re-do the analysis up to that point.

Let's now create our visuals:

R
<pre style="color: silver; background:black;">&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;
&num; Exploratory data analysis of RNAseq data with DESeq2
&num;
&num; these next R scripts are for a variety of visualization, QC and other plots to
&num; get a sense of what the RNAseq data looks like based on DESEq2 analysis
&num;
&num; 1) MA plot
&num; 2) rlog stabilization and variance stabiliazation
&num; 3) PCA plot
&num; 4) heatmap of clustering analysis
&num;
&num;
&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;&num;

&num; MA plot of RNAseq data for entire dataset
&num; http://en.wikipedia.org/wiki/MA_plot
&num; genes with padj < 0.1 are colored Red
plotMA(dds, ylim=c(-8,8),main = "RNAseq experiment")
<img src= "images/Croaker_DESeq2-MAplot_initial_analysis.png" alt="MA Plot">
dev.copy(png, paste0(outputPrefix, "-MAplot_initial_analysis.png"))
dev.off()

&num; transform raw counts into normalized values
&num; DESeq2 has two options:  1) rlog transformed and 2) variance stabilization
&num; variance stabilization is very good for heatmaps, etc.
&num; for an explanation as to what these are, please visit <a href="https://github.com/wolf-adam-eily/how_deseq2_works#how_deseq2_works_7">here (vsd)</a> and <a href="https://rdrr.io/bioc/DESeq2/man/rlog.html">here (rlog)</a>
rld <- rlogTransformation(dds, blind=T)
vsd <- varianceStabilizingTransformation(dds, blind=T)

&num; the option 'blind=T' tells DESeq2 that you want the procedure to be done without regard to the sample condition

&num; this option should be exercised with caution, as both of the functions rely on the variance of the idealized distribution

&num; should you set 'blind=T' for samples containing different cell types, for sample, the idealized variances would become much larger

&num; than they should because the data itself is not similar at all!

&num; save normalized values
write.table(as.data.frame(assay(rld),file = paste0(outputPrefix, "-rlog-transformed-counts.txt"), sep = '\t'))
write.table(as.data.frame(assay(vsd),file = paste0(outputPrefix, "-vst-transformed-counts.txt"), sep = '\t'))


&num; clustering analysis
&num; excerpts from http://dwheelerau.com/2014/02/17/how-to-use-deseq2-to-analyse-rnaseq-data/
library("RColorBrewer")
library("gplots")</pre>

For our heatmap we use the "dist" function to calculate the distance between datapoints (simply the difference of their values. We use the "assay" argument to tell R to calculate the distancee between each indiviidual assay (sample) and not the genes. Lastly, always transpose your expression matrix when calculating distances! The reason for this is that R's arithmetic operates on rows, not columns. So if we want to view each sample completely, we need the genes in the columns and the samples as the rows. Have you ever noticed how there is a rowMeans() function but not a colMeans() function? This is what I mean. We are assuming that you are familiar with how to use the heatmap function in R

<pre style="color: silver; background:black;">
sampleDists <- dist(t(assay(rld)))
suppressMessages(library("RColorBrewer"))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(colnames(rld), rld$type, sep="")
colnames(sampleDistMatrix) <- paste(colnames(rld), rld$type, sep="")
colors <- colorRampPalette( rev(brewer.pal(8, "Blues")) )(255)
heatmap(sampleDistMatrix,col=colors,margin = c(8,8))
<img src= "Croaker_DESeq2-clustering.png" alt = "Cluster Heatmap">
dev.copy(png,paste0(outputPrefix, "-clustering.png"))
dev.off()

&num; Principal components plot shows additional but rough clustering of samples

library("genefilter")
library("ggplot2")
library("grDevices")

&num; the rowVars() function calculates thee variance of each row

&num; we want the variance for each gene, not sample, so we will not be transposing our data

rv <- rowVars(assay(rld))

&num; we order our genes by those with maximum variance to minimum variance and take the first 500 genes in this order
select <- order(rv, decreasing=T)[seq_len(min(500,length(rv)))]

&num; we use the 'prcomp()' function to calculate the prinicpal components of our 500 genes. Notice that we want to group our samples,

&num; not genes. Therefore, we transpose our data

pc <- prcomp(t(assay(rld)[select,]))

&num; here is a look at the output of prcomp()

<strnog>Value</strong>

prcomp returns a list with class "prcomp" containing the following components:

sdev		the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the 
		covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).
rotation	the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns 
		this in the element loadings.
x		if retx is true the value of the rotated data (the centred (and scaled if requested) data multiplied by the rotation 
		matrix) is returned. Hence, cov(x) is the diagonal matrix diag(sdev^2). For the formula method, napredict() is applied 
		to handle the treatment of values omitted by the na.action.
center, scale	the centering and scaling used, or FALSE.

head(pc$x)
<strong> PC1        PC2        PC3           PC4
LB2A_1 -14.71439  0.4859991  2.2859939 -1.592476e-15
LB2A_2 -14.71617 -0.4412225 -2.2908771  3.694961e-15
LC2A_1  14.92945  3.0521331 -0.3425988  1.630987e-14
LC2A_2  14.50110 -3.0969098  0.3474820  1.682682e-14</strong>

&num; we see that x has our principal components!



&num; we will be using ggplot

&num; ggplot can only extract information from dataframes. Therefore, we will be making a dataframe of x and our conditions

scores <- data.frame(pc$x, sampleCondition)
head(scores)
<strong>> head(scores)
             PC1        PC2        PC3           PC4 sampleCondition
LB2A_1 -14.71439  0.4859991  2.2859939 -1.592476e-15         control
LB2A_2 -14.71617 -0.4412225 -2.2908771  3.694961e-15         control
LC2A_1  14.92945  3.0521331 -0.3425988  1.630987e-14         treated
LC2A_2  14.50110 -3.0969098  0.3474820  1.682682e-14         treated</strong>

&num; let's create our PCA plot, do not worry too much about the options

&num; you can design your own plot using the <a href="https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf">ggplot2 cheat sheet</a>

pcaplot <- ggplot(scores, aes(x = PC1, y = PC2, col = (factor(condition))))
  + geom_point(size = 5)
  + ggtitle("Principal Components")
  + scale_colour_brewer(name = " ", palette = "Set1")
  + theme(
    plot.title = element_text(face = 'bold'),
    legend.position = c(.9,.2),
    legend.key = element_rect(fill = 'NA'),
    legend.text = element_text(size = 10, face = "bold"),
    axis.text.y = element_text(colour = "Black"),
    axis.text.x = element_text(colour = "Black"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = 'bold'),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.background = element_rect(color = 'black',fill = NA)
  ))
<img src="images/Croaker_DESeq2-ggplot2.png" alt= "PCA Plot">
&num;dev.copy(png,paste0(outputPrefix, "-PCA.png"))
ggsave(pcaplot,file=paste0(outputPrefix, "-ggplot2.png"))



&num; let's create one more heatmap, but this time of our cleaned data

library("RColorBrewer")
library("gplots")
par(mar=c(7,4,4,2)+0.1) 
&num; 1000 top expressed genes with heatmap.2
select <- order(rowMeans(counts(ddsClean,normalized=T)),decreasing=T)[1:100]
my_palette <- colorRampPalette(c("blue",'white','red'))(n=100)
heatmap.2(assay(vsd)[select,], col=my_palette,
          scale="row", key=T, keysize=1, symkey=T,
          density.info="none", trace="none",
          cexCol=0.6, labRow=F,
          main="Heatmap of 100 DE Genes \nin Liver Tissue Comparison")
<img src="images/Croaker_DESeq2-HEATMAP.png" alt= "HEATMAP">
dev.copy(png, paste0(outputPrefix, "-HEATMAP.png"))
dev.off()</pre>

It is recommended the user study and attempt the code on one's own before moving onward. The resulting files are located in the directory.

<h2 id = "EnTAP">Entap: Functional Annotation for Transcriptomes</h2>

Unfortunately for those proceeding through this tutorial locally, an HPC is required to complete the annotation. We will be using the program EnTAP, which serves as a functional annotation for transcriptomes. The installation for EnTAP has not been included in this tutorial as the software must be installed on the server through which one wishes to run her or his commands. The installation file <i>does</i>, however, contain all of the dependcies, with the exception of the InterProScan and EGGNOG-MAPPER databases due to their sizes.

For robust transcriptomic data, functional annotation is a good avenue through which to begin annotating genomes. The core step of functional annotation is creating alignments to a genomic-data-base, retrieving the function and ontology of the alignments to be used to annotate the genome of interest. To do this, a genomic-data-base must be curated, indexed, followed by a series of filtering (contaminant detection, assembly artifact removal) and our alignment steps. While we will not be covering the ins-and-outs of EnTAP specifically here, you may visit http://entap.readthedocs.io/en/latest/introduction.html for insight on the methodology of the software.

Before running EnTAP we must complete a series of needed local 'warm-up' tasks first, starting with retrieving the protein IDs of our differentially expressed genes. Let's begin this objective by moving our first 9 differentially expressed genes from our Croaker_DESeq2-results-with-normalized.csv file to a temporary file, temp.csv, then use those 9 gene ids to retrieve the 9 corresponding protein ids and AA-sequences from the protein table found at https://www.ncbi.nlm.nih.gov/genome/proteins/12197?genome_assembly_id=229515 (this must be downloaded now). To do this, we will use csvgeneID2fasta.py with the following code:

<pre style="color: silver; background: black;">head -n 10 Croaker_DESeq2-results-with-normalized.csv > temp.csv 
python csvgeneID2fasta.py
</pre>

Which will prompt us with a series of questions we must answer:

<pre style="color: silver; background: black;">Please enter the file destination containing your differentially expressed Gene IDs 
temp.csv
Please enter the file destination containing the appropriate NIH protein table 
ProteinTable12197_229515.txt
Please enter the file destination of your protein fasta 
GCF_000972845.1_L_crocea_1.0_protein.faa
Please enter your desired fasta output destination 
fasta_out.fasta</pre>

After generating our new fasta file, we must now create the databases against which we will be searching for our annotations. We will be using two sources for our databases: the Uniprot-Swissprot fasta, and the <a href="ftp://ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other/">vertebrate_other</a> amino-acd fastas provided through NCBI's RefSeq release. The Uniprot-Swissprot database is already loaded onto the <a href="https://bioinformatics.uconn.edu/databases">Xanadu<a/> server. However, the "vertebrate_other" fastas are not, so we will be retrieving and compiling the RefSeq database ourselves. Looking at the link we see that there are four types of files for each index. Because our "fasta_out" file has protein sequences, we are only interested in the amino acid fastas, the 'faa.gz' files. We may use the '-A' argument of wget (along with other arguments I encourage you to look up) to select only the amino acid fastas. To do this we run:

<pre style="color: silver; background: black;">wget -A faa.gz -m -p -E -k -K -np ftp://ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other/</pre>

Now we must compile all of the fastas into a single fasta:

<pre style="color: silver; background: black;">cd ftp.ncbi.nlm.nih.gov/refseq/release/vertebrate_other
gunzip &#42;.faa.gz
cat &#42;.faa > vertebrate_other_fasta.txt | tr '\n' ''</pre>

We use the UNIX 'tr' function to remove new lines. The reason for this will become evident with experience, but the RefSeq fastas are compiled in such a way that when concatenating, blank lines are appended between the ends and beginning of each file. These blank lines cause errors when creating our database, so it is necessary to remove them.

Now we must create scannable databases using the software <a href="https://github.com/bbuchfink/diamond">DIAMOND</a>. DIAMOND is renowned for the efficiency and speed with which it not only creates scannable databases, but also its alignment matching against those databases. DIAMOND's efficiency is such that it is runnable on most laptops and personal computers, which is truly quite marvelous considering its resource intensive alternative, BLAST. We initialize a script with our "nano" command and Slurm arguments with the coding portion reading:

<pre style="color: silver; background: black;">module load diamond
wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accesion2taxid/prot.accession2taxid.gz
wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip
diamond makedb --in vertebrate_other.fasta -d vertebrate_other --taxonmap prot.accession2taxid.gz --taxonnodes taxdmp.zip</pre>

makedb is run with the options --taxonmap and --taxonnodes to structure the databases such that should we prefer alignments made to a specific clade, such as chordates, the database contains the appropriate information to do as we ask.

Now we have all of the ingredients to write our EnTAP script. We now secure copy our appropriate files to our Xanadu directory. We are now ready to EnTAP!

<pre style="color: silver; background: black;">nano entap.sh


Required Flags:
--runP      with a protein input, frame selection will not be ran and annotation will be executed with protein sequences (blastp)
-i          Path to the transcriptome file (either nucleotide or protein)
-d          Specify up to 5 DIAMOND indexed (.dmnd) databases to run similarity search against

Optional:
-threads    Number of threads
--ontology  0 - EggNOG (default)</pre>

We now submit our EnTAP script to the Slurm scheduler and wait for the process to finish. Once the job is done it will create a folder called “outfiles” which will contain the output of the program:

<pre style="color: silver; background: black;">entap/
|-- outfiles/
|   |-- debug_2018.2.25-2h49m29s.txt
|   |-- entap_out/
|   |-- final_annotated.faa
|   |-- final_annotations_lvl0_contam.tsv
|   |-- final_annotations_lvl0_no_contam.tsv
|   |-- final_annotations_lvl0.tsv
|   |-- final_annotations_lvl3_contam.tsv
|   |-- final_annotations_lvl3_no_contam.tsv
|   |-- final_annotations_lvl3.tsv
|   |-- final_annotations_lvl4_contam.tsv
|   |-- final_annotations_lvl4_no_contam.tsv
|   |-- final_annotations_lvl4.tsv
|   |-- final_unannotated.faa
|   |-- final_unannotated.fnn
|   |-- log_file_2018.2.25-2h49m29s.txt
|   |-- ontology/
|   |-- similarity_search/</pre>

<h2 id = "Integration">Integrating the DE Results with the Annotation Results</h2>

You must copy the following two files from the EnTAP run to your computer using Globus Peronsal Connect: “GeneID_proteinID.txt” and “final_annotations_lvl0_contam.tsv” file from the previous run. The two files may be found in:

<pre style="color: silver; background: black;">
entap/
|-- GeneID_proteinID.txt
|-- outfiles/
|   |-- final_annotations_lvl0_contam.tsv</pre>



Now, lastly, we integrate the annotations with the DE genes using the following R code:

<pre style="color: silver; background: black;">library("readr")
&num;read the csv file with DE genes
csv <- read.csv("Croaker_DESeq2-results-with-normalized.csv")
&num;read the file with geneID to proteinID relationship
gene_protein_list <- read.table("GeneID_proteinID.txt")
names(gene_protein_list) <- c('GeneID','table', 'tableID','protein', 'protienID')
gene_protein_list <- gene_protein_list[,c("GeneID","protienID")]

&num;merging the two dataframes
DEgene_proteinID <- merge(csv, gene_protein_list, by.x="gene", by.y="GeneID")

&num;read_tsv
annotation_file <- read_tsv('final_annotations_lvl0.tsv', col_names = TRUE)
names(annotation_file)[1] <- 'query_seqID'

&num;merging the DEgene_proteinID with annotation_file dataframes
annotated_DEgenes <- merge(DEgene_proteinID, annotation_file, by.x="protienID", by.y="query_seqID")
View(annotated_DEgenes)
write.csv(annotated_DEgenes, file = paste0("annotated_DEgenes_final.csv"))</pre>

Congratulations on completing your differential expression and functional annotation tutorial!

<h2 id="Citation">Citations</h2>

Alex Hart, Jill Wegrzyn http://entap.readthedocs.io/en/latest/index.html

Anders, Simon, Paul Theodor Pyl, and Wolfgang Huber. “HTSeq—a Python Framework to Work with High-Throughput Sequencing Data.” Bioinformatics 31.2 (2015): 166–169. PMC. Web. 8 Mar. 2018.

B. Buchfink, Xie C., D. Huson, "Fast and sensitive protein alignment using DIAMOND", Nature Methods 12, 59-60 (2015).

E. Neuwirth, RColorBrewer https://cran.r-project.org/web/packages/RColorBrewer/index.html

Gentleman R, Carey V, Huber W and Hahne F (2017). genefilter: genefilter: methods for filtering genes from high-throughput experiments. R package version 1.60.0. 

Gregory R. Warnes, Ben Bolker, Lodewijk Bonebakker, Robert Gentleman, Wolfgang Huber Andy Liaw, Thomas Lumley, Martin Maechler, Arni Magnusson, Steffen Moeller, Marc Schwartz, Bill Venables, gplots https://cran.r-project.org/web/packages/gplots/index.html

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009. 

Joshi NA, Fass JN. (2011). Sickle: A sliding-window, adaptive, quality-based trimming tool for FastQ files 
(Version 1.33) [Software].  Available at https://github.com/najoshi/sickle.

Leinonen, Rasko, Hideaki Sugawara, and Martin on behalf of the International Nucleotide Sequence Database Collaboration. “The Sequence Read Archive.” Nucleic Acids Research 39.Database issue (2011): D19–D21. PMC. Web. 8 Mar. 2018.

Li H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R, and 1000 Genome Project Data Processing Subgroup, The Sequence alignment/map (SAM) format and SAMtools, Bioinformatics (2009) 25(16) 2078-9

Love MI, Huber W and Anders S (2014). “Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2.” Genome Biology, 15, pp. 550. doi: 10.1186/s13059-014-0550-8. 
